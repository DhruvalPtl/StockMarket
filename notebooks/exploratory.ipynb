{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d2c97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Groww!\n",
      "Logged into Groww!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from growwapi import GrowwAPI\n",
    "\n",
    "# --- CREDENTIALS ---\n",
    "API_KEY = \"eyJraWQiOiJaTUtjVXciLCJhbGciOiJFUzI1NiJ9.eyJleHAiOjI1NTQzNTIwMzYsImlhdCI6MTc2NTk1MjAzNiwibmJmIjoxNzY1OTUyMDM2LCJzdWIiOiJ7XCJ0b2tlblJlZklkXCI6XCI3NzljMTAyNy03ZDQ1LTRlOWItYWM5ZS1iNDgzMWRiODQzZTFcIixcInZlbmRvckludGVncmF0aW9uS2V5XCI6XCJlMzFmZjIzYjA4NmI0MDZjODg3NGIyZjZkODQ5NTMxM1wiLFwidXNlckFjY291bnRJZFwiOlwiMDdmMDA0MGMtZTk4Zi00ZDNmLTk5Y2EtZDc1ZjBlYWU5M2NlXCIsXCJkZXZpY2VJZFwiOlwiZDMyMWIxMzUtZWQ5Mi01ZWJkLWJjMDUtZTY1NDY2OWRiMDM5XCIsXCJzZXNzaW9uSWRcIjpcIjllODBhNjM2LTY4OGMtNDQ4OC1hMDhjLTU1NzQwMDQwNDMwZlwiLFwiYWRkaXRpb25hbERhdGFcIjpcIno1NC9NZzltdjE2WXdmb0gvS0EwYk1yOE5XVzhzdTNvZ080am1ZUzIwZEpSTkczdTlLa2pWZDNoWjU1ZStNZERhWXBOVi9UOUxIRmtQejFFQisybTdRPT1cIixcInJvbGVcIjpcImF1dGgtdG90cFwiLFwic291cmNlSXBBZGRyZXNzXCI6XCIyNDA5OjQwOTA6MTA4ZjpkYzA1OmM5ODU6OWEzNjo2ZTEyOjFjZWIsMTYyLjE1OC4yMzUuMjA0LDM1LjI0MS4yMy4xMjNcIixcInR3b0ZhRXhwaXJ5VHNcIjoyNTU0MzUyMDM2MzA0fSIsImlzcyI6ImFwZXgtYXV0aC1wcm9kLWFwcCJ9.vFYYnOrSLi-teVY6qhFF11SeSVZRIo-xBz_lVlOoTDujYw3ucWZSbOoP9sqFg11Oc8cCwWASqbg_R-9BfmPU0Q\"\n",
    "API_SECRET = \"Gb!4#@-d4*XbNz)F!)Y0iW8122uJiaTn\"\n",
    "\n",
    "def auth():\n",
    "    try:\n",
    "        token = GrowwAPI.get_access_token(api_key=API_KEY, secret=API_SECRET)\n",
    "        return GrowwAPI(token)\n",
    "    except Exception as e:\n",
    "        print(f\"Auth failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "groww = auth()\n",
    "print(\"Logged into Groww!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03aceccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched download for NSE-NIFTY (CASH)...\n",
      "Fetching: 2025-12-15 12:21:02 -> 2025-12-17 12:21:02\n",
      "  > Success: 166 candles.\n",
      "\n",
      "DONE! Saved 166 candles to nifty_live.csv\n",
      "First 5 rows:\n",
      "            timestamp      open      high       low     close volume  \\\n",
      "0 2025-12-15 12:20:00  26023.05  26025.75  26020.75  26023.40   None   \n",
      "1 2025-12-15 12:25:00  26023.70  26025.85  26011.45  26018.60   None   \n",
      "2 2025-12-15 12:30:00  26020.10  26027.65  26017.45  26025.60   None   \n",
      "3 2025-12-15 12:35:00  26025.85  26029.65  26019.95  26022.20   None   \n",
      "4 2025-12-15 12:40:00  26021.85  26024.30  26007.70  26010.15   None   \n",
      "\n",
      "  open_interest  \n",
      "0          None  \n",
      "1          None  \n",
      "2          None  \n",
      "3          None  \n",
      "4          None  \n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# The new method requires these specific strings\n",
    "SYMBOL = \"NSE-NIFTY\" \n",
    "EXCHANGE = \"NSE\"          # Passed as string\n",
    "SEGMENT = \"CASH\"       # Passed as string (\"INDICES\" for Nifty 50, \"CASH\" for stocks)\n",
    "INTERVAL = \"5minute\"           # Passed as string (e.g., \"1m\", \"5m\", \"15m\")\n",
    "\n",
    "# --- CHUNKING LOGIC ---\n",
    "total_days = 2\n",
    "chunk_size_days = 5\n",
    "end_date = pd.Timestamp.now()\n",
    "start_date = end_date - timedelta(days=total_days)\n",
    "\n",
    "all_candles = []\n",
    "current_start = start_date\n",
    "\n",
    "print(f\"Starting batched download for {SYMBOL} ({SEGMENT})...\")\n",
    "\n",
    "while current_start < end_date:\n",
    "    current_end = current_start + timedelta(days=chunk_size_days)\n",
    "    if current_end > end_date:\n",
    "        current_end = end_date\n",
    "    \n",
    "    # Format dates to string as required by signature\n",
    "    s_str = current_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    e_str = current_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    print(f\"Fetching: {s_str} -> {e_str}\")\n",
    "    \n",
    "    try:\n",
    "        # UPDATED CALL MATCHING YOUR SIGNATURE\n",
    "        resp = groww.get_historical_candles(\n",
    "            exchange=EXCHANGE,\n",
    "            segment=SEGMENT,\n",
    "            groww_symbol=SYMBOL,\n",
    "            start_time=s_str,\n",
    "            end_time=e_str,\n",
    "            candle_interval=INTERVAL\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        if resp and \"candles\" in resp and resp[\"candles\"]:\n",
    "            chunk_data = resp[\"candles\"]\n",
    "            all_candles.extend(chunk_data)\n",
    "            print(f\"  > Success: {len(chunk_data)} candles.\")\n",
    "        else:\n",
    "            # Sometimes API returns empty or wrapped data, print to debug if needed\n",
    "            print(f\"  > No data/Empty response. (Check if Market was open)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  > Error: {e}\")\n",
    "\n",
    "    current_start = current_end\n",
    "    time.sleep(0.5) # Rate limit protection\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "# --- SAVE TO CSV ---\n",
    "if all_candles:\n",
    "    # 1. Create DataFrame\n",
    "    df = pd.DataFrame(all_candles)\n",
    "    \n",
    "    # 2. Rename Columns (Handle optional 7th column)\n",
    "    base_cols = [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    rename_map = {i: col for i, col in enumerate(base_cols)}\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    if len(df.columns) > 6:\n",
    "        df.rename(columns={6: \"open_interest\"}, inplace=True)\n",
    "    \n",
    "    # 3. ROBUST TIMESTAMP FIX\n",
    "    # Check the first value to decide how to convert\n",
    "    first_ts = df[\"timestamp\"].iloc[0]\n",
    "    \n",
    "    if isinstance(first_ts, str):\n",
    "        # Case A: API returns strings like '2025-11-07T13:43:00'\n",
    "        # Do NOT use unit='s' here.\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    else:\n",
    "        # Case B: API returns numbers (Epoch seconds) like 1735660000\n",
    "        # MUST use unit='s' here.\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit='s')\n",
    "    \n",
    "    # 4. Clean & Save\n",
    "    df.sort_values(\"timestamp\", inplace=True)\n",
    "    df.drop_duplicates(subset=\"timestamp\", inplace=True)\n",
    "    \n",
    "    filename = \"nifty_live.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"\\nDONE! Saved {len(df)} candles to {filename}\")\n",
    "    print(\"First 5 rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\nFAILED: No data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4fb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The script already does this:\n",
    "df.drop(columns=['volume', 'open_interest'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00760d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Generating Technical Indicators...\n",
      "Applying Fractional Differentiation (This might take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUCCESS! Processed data saved to nifty_training_data.csv\n",
      "Features ready: ['open', 'high', 'low', 'close', 'RSI', 'MACD', 'MACD_Signal', 'BB_High', 'BB_Low', 'FracDiff_Close', 'Target_Return', 'Target']\n",
      "                         open      high       low     close        RSI  \\\n",
      "timestamp                                                                \n",
      "2025-11-07 14:56:00  25500.65  25503.60  25498.95  25500.50  42.884080   \n",
      "2025-11-07 14:57:00  25499.70  25504.05  25493.05  25493.60  35.420975   \n",
      "2025-11-07 14:58:00  25493.20  25493.45  25486.75  25488.25  30.926820   \n",
      "2025-11-07 14:59:00  25487.50  25490.55  25484.60  25490.10  34.043195   \n",
      "2025-11-07 15:00:00  25492.50  25496.75  25489.75  25492.85  38.486026   \n",
      "\n",
      "                         MACD  MACD_Signal       BB_High        BB_Low  \\\n",
      "timestamp                                                                \n",
      "2025-11-07 14:56:00 -2.746876    -1.087647  25521.563339  25492.671661   \n",
      "2025-11-07 14:57:00 -3.248718    -1.519861  25520.580397  25491.214603   \n",
      "2025-11-07 14:58:00 -4.031657    -2.022220  25519.847783  25488.952217   \n",
      "2025-11-07 14:59:00 -4.451547    -2.508086  25518.943205  25487.341795   \n",
      "2025-11-07 15:00:00 -4.510418    -2.908552  25517.990695  25486.349305   \n",
      "\n",
      "                     FracDiff_Close  Target_Return  Target  \n",
      "timestamp                                                   \n",
      "2025-11-07 14:56:00      125.655453         -17.60       0  \n",
      "2025-11-07 14:57:00      118.855962         -13.45       0  \n",
      "2025-11-07 14:58:00      119.506636         -14.00       0  \n",
      "2025-11-07 14:59:00      126.401961          -8.45       0  \n",
      "2025-11-07 15:00:00      127.964173         -12.90       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ta  # Technical Analysis library (Works on Python 3.14)\n",
    "from tsfracdiff import FractionalDifferentiator \n",
    "\n",
    "# 1. Load your raw data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv('nifty_spot_1m.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: nifty_spot_1m.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Cleanup\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "df.drop(columns=['volume', 'open_interest'], errors='ignore', inplace=True)\n",
    "\n",
    "# 3. Feature Engineering: Technical Indicators (Using 'ta' lib)\n",
    "print(\"Generating Technical Indicators...\")\n",
    "\n",
    "# RSI (Momentum)\n",
    "# Initialize RSI Indicator\n",
    "rsi_indicator = ta.momentum.RSIIndicator(close=df['close'], window=14)\n",
    "df['RSI'] = rsi_indicator.rsi()\n",
    "\n",
    "# MACD (Trend)\n",
    "# Initialize MACD Indicator\n",
    "macd_indicator = ta.trend.MACD(close=df['close'])\n",
    "df['MACD'] = macd_indicator.macd()\n",
    "df['MACD_Signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "# Bollinger Bands (Volatility)\n",
    "# Initialize Bollinger Bands Indicator\n",
    "bb_indicator = ta.volatility.BollingerBands(close=df['close'], window=20, window_dev=2)\n",
    "df['BB_High'] = bb_indicator.bollinger_hband()\n",
    "df['BB_Low'] = bb_indicator.bollinger_lband()\n",
    "\n",
    "# 4. The \"Secret Sauce\": Fractional Differentiation\n",
    "print(\"Applying Fractional Differentiation (This might take a moment)...\")\n",
    "\n",
    "# We only differentiate the 'close' price for the AI to learn\n",
    "frac_diff = FractionalDifferentiator()\n",
    "res = frac_diff.FitTransform(df['close']) \n",
    "df['FracDiff_Close'] = res\n",
    "\n",
    "# 5. Create Targets (What we want to predict)\n",
    "# Goal: Will price go UP in the next 5 minutes?\n",
    "df['Target_Return'] = df['close'].shift(-5) - df['close'] \n",
    "df['Target'] = (df['Target_Return'] > 0).astype(int)\n",
    "\n",
    "# Drop NaNs created by indicators/shifting\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 6. Save Final Training Data\n",
    "output_file = 'nifty_training_data.csv'\n",
    "df.to_csv(output_file)\n",
    "print(f\"\\nSUCCESS! Processed data saved to {output_file}\")\n",
    "print(f\"Features ready: {list(df.columns)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c05cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Spot Data...\n",
      "Found 19 unique strikes touched: [np.int64(25450), np.int64(25500), np.int64(25550), np.int64(25600), np.int64(25650), np.int64(25700), np.int64(25750), np.int64(25800), np.int64(25850), np.int64(25900), np.int64(25950), np.int64(26000), np.int64(26050), np.int64(26100), np.int64(26150), np.int64(26200), np.int64(26250), np.int64(26300), np.int64(26350)]\n",
      "--------------------------------------------------\n",
      "Ready to Groww!\n",
      "Downloading History for NIFTY25D0925450CE...\n",
      "  > Saved 454 rows to options_data/NIFTY25D0925450CE.csv\n",
      "Downloading History for NIFTY25D0925450PE...\n",
      "  > Saved 454 rows to options_data/NIFTY25D0925450PE.csv\n",
      "Downloading History for NIFTY25D0925500CE...\n",
      "  > Saved 454 rows to options_data/NIFTY25D0925500CE.csv\n",
      "Downloading History for NIFTY25D0925500PE...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_end > end_dt: current_end = end_dt\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Use the new method signature you discovered\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     resp = \u001b[43mgroww\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_historical_candles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexchange\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNSE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFNO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Options are in FNO segment\u001b[39;49;00m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroww_symbol\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNSE-NIFTY-09Dec25-25450-CE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_start\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mY-\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[33;43m \u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mH:\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mM:\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_end\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mY-\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[33;43m \u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mH:\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mM:\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandle_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1minute\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcandles\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp \u001b[38;5;129;01mand\u001b[39;00m resp[\u001b[33m\"\u001b[39m\u001b[33mcandles\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     90\u001b[39m         all_candles.extend(resp[\u001b[33m\"\u001b[39m\u001b[33mcandles\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\growwapi\\groww\\client.py:912\u001b[39m, in \u001b[36mGrowwAPI.get_historical_candles\u001b[39m\u001b[34m(self, exchange, segment, groww_symbol, start_time, end_time, candle_interval, timeout)\u001b[39m\n\u001b[32m    903\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.domain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/historical/candles\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m params = {\n\u001b[32m    905\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexchange\u001b[39m\u001b[33m\"\u001b[39m: exchange,\n\u001b[32m    906\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m: segment,\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcandle_interval\u001b[39m\u001b[33m\"\u001b[39m: candle_interval,\n\u001b[32m    911\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\growwapi\\groww\\client.py:1405\u001b[39m, in \u001b[36mGrowwAPI._request_get\u001b[39m\u001b[34m(self, url, params, headers, timeout, **kwargs)\u001b[39m\n\u001b[32m   1388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1389\u001b[39m \u001b[33;03mSend a GET request to the API.\u001b[39;00m\n\u001b[32m   1390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1402\u001b[39m \u001b[33;03m    GrowwAPIException: If the request fails.\u001b[39;00m\n\u001b[32m   1403\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.Timeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GrowwAPITimeoutException() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\urllib3\\connection.py:796\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    793\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    794\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    814\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    816\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\urllib3\\connection.py:975\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    973\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\StockMarket\\StockMarket\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:461\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from growwapi import GrowwAPI\n",
    "\n",
    "# --- CREDENTIALS ---\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "API_SECRET = \"YOUR_API_SECRET_HERE\"\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# 1. EXPIRY DATE STRING\n",
    "# User confirmed format is \"09Dec25\" (Day + ShortMonth + ShortYear)\n",
    "# Change this to match your target expiry (e.g., \"26Dec25\" for monthly)\n",
    "EXPIRY_DATE_STR = \"09Dec25\" \n",
    "\n",
    "def auth():\n",
    "    try:\n",
    "        token = GrowwAPI.get_access_token(api_key=API_KEY, secret=API_SECRET)\n",
    "        return GrowwAPI(token)\n",
    "    except Exception as e:\n",
    "        print(f\"Auth failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "# 1. Load Spot Data\n",
    "print(\"Loading Spot Data...\")\n",
    "try:\n",
    "    spot_df = pd.read_csv('nifty_spot_1m.csv')\n",
    "    # Standardize timestamp\n",
    "    if isinstance(spot_df['timestamp'].iloc[0], str):\n",
    "        spot_df['timestamp'] = pd.to_datetime(spot_df['timestamp'])\n",
    "    else:\n",
    "        spot_df['timestamp'] = pd.to_datetime(spot_df['timestamp'], unit='s')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: nifty_spot_1m.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Identify Unique ATM Strikes\n",
    "# Round to nearest 50\n",
    "spot_df['ATM_Strike'] = (spot_df['close'] / 50).round() * 50\n",
    "unique_strikes = sorted(spot_df['ATM_Strike'].unique().astype(int))\n",
    "\n",
    "print(f\"Found {len(unique_strikes)} unique strikes: {unique_strikes}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "groww = auth()\n",
    "start_dt = spot_df['timestamp'].min()\n",
    "end_dt = spot_df['timestamp'].max()\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"options_data\", exist_ok=True)\n",
    "\n",
    "# 3. Download Loop\n",
    "for strike in unique_strikes:\n",
    "    # --- NEW SYMBOL GENERATION LOGIC ---\n",
    "    # Format: NSE-NIFTY-09Dec25-25450-CE\n",
    "    ce_symbol = f\"NSE-NIFTY-{EXPIRY_DATE_STR}-{strike}-CE\"\n",
    "    pe_symbol = f\"NSE-NIFTY-{EXPIRY_DATE_STR}-{strike}-PE\"\n",
    "    \n",
    "    symbols_to_fetch = [ce_symbol, pe_symbol]\n",
    "    \n",
    "    for sym in symbols_to_fetch:\n",
    "        filename = f\"options_data/{sym}.csv\"\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Skipping {sym} (Already exists)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Downloading History for {sym}...\")\n",
    "        \n",
    "        current_start = start_dt\n",
    "        all_candles = []\n",
    "        \n",
    "        while current_start < end_dt:\n",
    "            current_end = current_start + timedelta(days=5)\n",
    "            if current_end > end_dt: current_end = end_dt\n",
    "            \n",
    "            try:\n",
    "                # DYNAMIC CALL (No longer hardcoded!)\n",
    "                resp = groww.get_historical_candles(\n",
    "                    exchange=\"NSE\",\n",
    "                    segment=\"FNO\", \n",
    "                    groww_symbol=sym,  # <--- Using the dynamic variable here\n",
    "                    start_time=current_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    end_time=current_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    candle_interval=\"1minute\"\n",
    "                )\n",
    "                \n",
    "                if resp and \"candles\" in resp and resp[\"candles\"]:\n",
    "                    all_candles.extend(resp[\"candles\"])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  > Error fetching chunk: {e}\")\n",
    "            \n",
    "            current_start = current_end\n",
    "            time.sleep(0.5) \n",
    "            \n",
    "        # Save if we got data\n",
    "        if all_candles:\n",
    "            df = pd.DataFrame(all_candles)\n",
    "            \n",
    "            # Fix Columns\n",
    "            base_cols = [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "            rename_map = {i: col for i, col in enumerate(base_cols)}\n",
    "            df.rename(columns=rename_map, inplace=True)\n",
    "            if len(df.columns) > 6: df.rename(columns={6: \"oi\"}, inplace=True)\n",
    "            \n",
    "            # Fix Timestamp\n",
    "            if isinstance(df[\"timestamp\"].iloc[0], str):\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "            else:\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit='s')\n",
    "\n",
    "            df.drop_duplicates(subset=\"timestamp\", inplace=True)\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"  > Saved {len(df)} rows to {filename}\")\n",
    "        else:\n",
    "            print(f\"  > NO DATA for {sym}\")\n",
    "\n",
    "print(\"\\nDownload Complete. Check the 'options_data' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6044de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Groww!\n",
      "Logged into Groww!\n",
      "Targeting Expiry: 18Dec25\n",
      "Starting Collection... (Saving to option_ltp_1m.csv)\n",
      "Error fetching Spot: GrowwAPI.get_ltp() missing 2 required positional arguments: 'exchange_trading_symbols' and 'segment'\n",
      "[2025-12-17 12:39:15] Error: Could not fetch Nifty Spot\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from growwapi import GrowwAPI\n",
    "\n",
    "API_KEY = \"eyJraWQiOiJaTUtjVXciLCJhbGciOiJFUzI1NiJ9.eyJleHAiOjI1NTM0ODUwMTcsImlhdCI6MTc2NTA4NTAxNywibmJmIjoxNzY1MDg1MDE3LCJzdWIiOiJ7XCJ0b2tlblJlZklkXCI6XCJmYjg0YzJmOS04NGUwLTQ2NGMtYWFkZC0wZjMyZTBiNDZmY2FcIixcInZlbmRvckludGVncmF0aW9uS2V5XCI6XCJlMzFmZjIzYjA4NmI0MDZjODg3NGIyZjZkODQ5NTMxM1wiLFwidXNlckFjY291bnRJZFwiOlwiMDdmMDA0MGMtZTk4Zi00ZDNmLTk5Y2EtZDc1ZjBlYWU5M2NlXCIsXCJkZXZpY2VJZFwiOlwiZDMyMWIxMzUtZWQ5Mi01ZWJkLWJjMDUtZTY1NDY2OWRiMDM5XCIsXCJzZXNzaW9uSWRcIjpcIjVlZDUwZmU2LTBiNjktNDBlMC04ZDJmLTJlZjE3Y2YxZDYwN1wiLFwiYWRkaXRpb25hbERhdGFcIjpcIno1NC9NZzltdjE2WXdmb0gvS0EwYk1yOE5XVzhzdTNvZ080am1ZUzIwZEpSTkczdTlLa2pWZDNoWjU1ZStNZERhWXBOVi9UOUxIRmtQejFFQisybTdRPT1cIixcInJvbGVcIjpcImF1dGgtdG90cFwiLFwic291cmNlSXBBZGRyZXNzXCI6XCIyNDA5OjQwOTA6MTA4ZjpkYzA1OjFjNDg6YTliYjo5MTZiOmI4NWQsMTcyLjcxLjE5OC4xMjgsMzUuMjQxLjIzLjEyM1wiLFwidHdvRmFFeHBpcnlUc1wiOjI1NTM0ODUwMTc4ODR9IiwiaXNzIjoiYXBleC1hdXRoLXByb2QtYXBwIn0.GCoXAEdA0BkhB88lQmsYqzl96qaGudoM3UvzHxEh_tGfODPmrLzTNPMo8KCeTpzwf46Hp-wU41QxjNPwGyHmag\"\n",
    "API_SECRET = \"F@ldixy2hTCYKBq30fyNIyz#PaJ1Ui9i\"\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INDEX_SYMBOL = \"NIFTY\"\n",
    "CSV_FILE = \"option_ltp_1m.csv\"\n",
    "SLEEP_SECONDS = 60\n",
    "\n",
    "# Automatically find next Thursday for the symbol format\n",
    "def get_next_expiry_format():\n",
    "    \"\"\"\n",
    "    Returns the next Thursday in 'DDMonYY' format (e.g., '09Dec25').\n",
    "    \"\"\"\n",
    "    today = datetime.date.today()\n",
    "    # 0=Mon, 3=Thu\n",
    "    days_ahead = 3 - today.weekday()\n",
    "    if days_ahead <= 0: \n",
    "        # If today is Thu (0) or later...\n",
    "        # If today is Thu, use today. If Fri/Sat/Sun, go to next week.\n",
    "        if days_ahead < 0:\n",
    "            days_ahead += 7\n",
    "            \n",
    "    next_thursday = today + datetime.timedelta(days=days_ahead)\n",
    "    # Format: 09Dec25 (%d%b%y)\n",
    "    return next_thursday.strftime(\"%d%b%y\")\n",
    "\n",
    "def auth():\n",
    "    try:\n",
    "        token = GrowwAPI.get_access_token(api_key=API_KEY, secret=API_SECRET)\n",
    "        return GrowwAPI(token)\n",
    "    except Exception as e:\n",
    "        print(f\"Auth failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "def get_nifty_spot(groww_client):\n",
    "    \"\"\"Fetches the live Nifty 50 Index value.\"\"\"\n",
    "    try:\n",
    "        # Fetching Index LTP usually requires a specific call or searching 'NSE-NIFTY'\n",
    "        # Method A: get_ltp with generic symbol\n",
    "        groww_client.get_ltp()\n",
    "        resp = groww_client.get_ltp(\n",
    "            exchange=groww_client.EXCHANGE_NSE,\n",
    "            segment=groww_client.SEGMENT_INDICES,\n",
    "            exchange_trading_symbols=[INDEX_SYMBOL]\n",
    "        )\n",
    "        # Parse: Resp is usually a list of dicts or a dict\n",
    "        if isinstance(resp, list) and len(resp) > 0:\n",
    "            return resp[0].get('ltp')\n",
    "        elif isinstance(resp, dict):\n",
    "            # Sometimes returns {'NIFTY': {'ltp': ...}}\n",
    "            val = list(resp.values())[0]\n",
    "            if isinstance(val, dict): return val.get('ltp')\n",
    "            return val\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Spot: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_option_ltp(groww_client, symbol):\n",
    "    \"\"\"Fetches LTP for a specific Option Symbol.\"\"\"\n",
    "    try:\n",
    "        # Using get_ltp for FNO segment\n",
    "        resp = groww_client.get_ltp(\n",
    "            exchange=groww_client.EXCHANGE_NSE,\n",
    "            segment=groww_client.SEGMENT_FNO,\n",
    "            exchange_trading_symbols=[symbol]\n",
    "        )\n",
    "        # Parse logic same as above\n",
    "        if isinstance(resp, list) and len(resp) > 0:\n",
    "            return resp[0].get('ltp')\n",
    "        elif isinstance(resp, dict):\n",
    "            val = list(resp.values())[0]\n",
    "            if isinstance(val, dict): return val.get('ltp')\n",
    "            return val\n",
    "    except Exception as e:\n",
    "        # Common error: Symbol doesn't exist (wrong expiry/strike)\n",
    "        print(f\"Error fetching Option {symbol}: {e}\")\n",
    "    return None\n",
    "\n",
    "# --- MAIN ---\n",
    "groww = auth()\n",
    "print(\"Logged into Groww!\")\n",
    "\n",
    "# Calculate Expiry String once (e.g. \"12Dec25\")\n",
    "# Verify this prints the date you expect!\n",
    "expiry_str = get_next_expiry_format() \n",
    "print(f\"Targeting Expiry: {expiry_str}\")\n",
    "\n",
    "# Initialize CSV if missing\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    pd.DataFrame(columns=[\"timestamp\", \"spot_price\", \"atm_strike\", \"ce_symbol\", \"ce_ltp\", \"pe_symbol\", \"pe_ltp\"]).to_csv(CSV_FILE, index=False)\n",
    "\n",
    "print(f\"Starting Collection... (Saving to {CSV_FILE})\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # 1. Fetch Spot\n",
    "        spot_ltp = get_nifty_spot(groww)\n",
    "        \n",
    "        if spot_ltp:\n",
    "            # 2. Determine ATM\n",
    "            atm_strike = round(spot_ltp / 50) * 50\n",
    "            \n",
    "            # 3. Construct Symbols\n",
    "            # Format: NSE-NIFTY-DDMonYY-Strike-Type\n",
    "            ce_sym = f\"NSE-NIFTY-{expiry_str}-{atm_strike}-CE\"\n",
    "            pe_sym = f\"NSE-NIFTY-{expiry_str}-{atm_strike}-PE\"\n",
    "            \n",
    "            # 4. Fetch Option LTPs\n",
    "            ce_ltp = get_option_ltp(groww, ce_sym)\n",
    "            pe_ltp = get_option_ltp(groww, pe_sym)\n",
    "            \n",
    "            # 5. Save Data\n",
    "            if ce_ltp is not None and pe_ltp is not None:\n",
    "                print(f\"[{now_str}] Spot: {spot_ltp} | ATM: {atm_strike} | CE: {ce_ltp} | PE: {pe_ltp}\")\n",
    "                \n",
    "                new_row = {\n",
    "                    \"timestamp\": now_str,\n",
    "                    \"spot_price\": spot_ltp,\n",
    "                    \"atm_strike\": atm_strike,\n",
    "                    \"ce_symbol\": ce_sym,\n",
    "                    \"ce_ltp\": ce_ltp,\n",
    "                    \"pe_symbol\": pe_sym,\n",
    "                    \"pe_ltp\": pe_ltp\n",
    "                }\n",
    "                \n",
    "                # Append to CSV\n",
    "                pd.DataFrame([new_row]).to_csv(CSV_FILE, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                print(f\"[{now_str}] Error: Could not fetch Option LTPs (Check Expiry/Symbol format)\")\n",
    "        else:\n",
    "            print(f\"[{now_str}] Error: Could not fetch Nifty Spot\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Loop Error: {e}\")\n",
    "        \n",
    "    time.sleep(SLEEP_SECONDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b605b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nifty_spot_1m.csv...\n",
      "Assigning Expiry Dates...\n",
      "Expiries required: ['23Dec25']\n",
      "Ready to Groww!\n",
      "\n",
      "--- Downloading Contracts ---\n",
      "\n",
      "Processing Expiry: 23Dec25 (Unique Strikes: 375)\n",
      "  Fetching NSE-NIFTY-23Dec25-12200-CE..."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from growwapi import GrowwAPI\n",
    "\n",
    "# --- CREDENTIALS ---\n",
    "API_KEY = \"eyJraWQiOiJaTUtjVXciLCJhbGciOiJFUzI1NiJ9.eyJleHAiOjI1NTQzNTIwMzYsImlhdCI6MTc2NTk1MjAzNiwibmJmIjoxNzY1OTUyMDM2LCJzdWIiOiJ7XCJ0b2tlblJlZklkXCI6XCI3NzljMTAyNy03ZDQ1LTRlOWItYWM5ZS1iNDgzMWRiODQzZTFcIixcInZlbmRvckludGVncmF0aW9uS2V5XCI6XCJlMzFmZjIzYjA4NmI0MDZjODg3NGIyZjZkODQ5NTMxM1wiLFwidXNlckFjY291bnRJZFwiOlwiMDdmMDA0MGMtZTk4Zi00ZDNmLTk5Y2EtZDc1ZjBlYWU5M2NlXCIsXCJkZXZpY2VJZFwiOlwiZDMyMWIxMzUtZWQ5Mi01ZWJkLWJjMDUtZTY1NDY2OWRiMDM5XCIsXCJzZXNzaW9uSWRcIjpcIjllODBhNjM2LTY4OGMtNDQ4OC1hMDhjLTU1NzQwMDQwNDMwZlwiLFwiYWRkaXRpb25hbERhdGFcIjpcIno1NC9NZzltdjE2WXdmb0gvS0EwYk1yOE5XVzhzdTNvZ080am1ZUzIwZEpSTkczdTlLa2pWZDNoWjU1ZStNZERhWXBOVi9UOUxIRmtQejFFQisybTdRPT1cIixcInJvbGVcIjpcImF1dGgtdG90cFwiLFwic291cmNlSXBBZGRyZXNzXCI6XCIyNDA5OjQwOTA6MTA4ZjpkYzA1OmM5ODU6OWEzNjo2ZTEyOjFjZWIsMTYyLjE1OC4yMzUuMjA0LDM1LjI0MS4yMy4xMjNcIixcInR3b0ZhRXhwaXJ5VHNcIjoyNTU0MzUyMDM2MzA0fSIsImlzcyI6ImFwZXgtYXV0aC1wcm9kLWFwcCJ9.vFYYnOrSLi-teVY6qhFF11SeSVZRIo-xBz_lVlOoTDujYw3ucWZSbOoP9sqFg11Oc8cCwWASqbg_R-9BfmPU0Q\"\n",
    "API_SECRET = \"Gb!4#@-d4*XbNz)F!)Y0iW8122uJiaTn\"\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SPOT_FILE = \"nifty_spot_1m.csv\"\n",
    "OUTPUT_FILE = \"option_ltp_1m_fsdfddfdfdfssdfrolling.csv\"\n",
    "\n",
    "# --- USER DEFINED EXPIRY SCHEDULE ---\n",
    "# We list your specific dates here (Year-Month-Day)\n",
    "# The script will pick the first date that is >= the current row's date.\n",
    "EXPIRY_SCHEDULE = [\n",
    "    pd.Timestamp(\"2025-12-23\")\n",
    "]\n",
    "\n",
    "def auth():\n",
    "    try:\n",
    "        token = GrowwAPI.get_access_token(api_key=API_KEY, secret=API_SECRET)\n",
    "        return GrowwAPI(token)\n",
    "    except Exception as e:\n",
    "        print(f\"Auth failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "def get_target_expiry(current_ts):\n",
    "    \"\"\"\n",
    "    Finds the correct expiry from the hardcoded list.\n",
    "    Logic: The nearest date in the list that is >= current_ts.\n",
    "    \"\"\"\n",
    "    current_date = current_ts.date()\n",
    "    for exp_ts in EXPIRY_SCHEDULE:\n",
    "        exp_date = exp_ts.date()\n",
    "        if exp_date >= current_date:\n",
    "            # Returns format: 04Nov25\n",
    "            return exp_ts.strftime(\"%d%b%y\")\n",
    "    return None\n",
    "\n",
    "# --- STEP 1: PREPARE SPOT DATA ---\n",
    "print(f\"Loading {SPOT_FILE}...\")\n",
    "try:\n",
    "    df_spot = pd.read_csv(SPOT_FILE)\n",
    "    if isinstance(df_spot['timestamp'].iloc[0], str):\n",
    "        df_spot['timestamp'] = pd.to_datetime(df_spot['timestamp'])\n",
    "    else:\n",
    "        df_spot['timestamp'] = pd.to_datetime(df_spot['timestamp'], unit='s')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {SPOT_FILE} not found.\")\n",
    "    exit()\n",
    "\n",
    "# 1. Calculate ATM Strike\n",
    "df_spot['atm_strike'] = (df_spot['close'] / 50).round() * 50\n",
    "\n",
    "# 2. Assign Correct Expiry to Every Minute\n",
    "print(\"Assigning Expiry Dates...\")\n",
    "df_spot['expiry_str'] = df_spot['timestamp'].apply(get_target_expiry)\n",
    "\n",
    "# Remove rows where no expiry was found (e.g. data after Dec 9)\n",
    "original_len = len(df_spot)\n",
    "df_spot.dropna(subset=['expiry_str'], inplace=True)\n",
    "if len(df_spot) < original_len:\n",
    "    print(f\"Warning: Dropped {original_len - len(df_spot)} rows that were past the last expiry date.\")\n",
    "\n",
    "print(\"Expiries required:\", df_spot['expiry_str'].unique())\n",
    "\n",
    "# --- STEP 2: DOWNLOAD BATCH HISTORY ---\n",
    "groww = auth()\n",
    "option_cache = {} \n",
    "grouped = df_spot.groupby('expiry_str')\n",
    "\n",
    "print(\"\\n--- Downloading Contracts ---\")\n",
    "\n",
    "for expiry, group in grouped:\n",
    "    unique_strikes = group['atm_strike'].unique().astype(int)\n",
    "    print(f\"\\nProcessing Expiry: {expiry} (Unique Strikes: {len(unique_strikes)})\")\n",
    "    \n",
    "    # We only need data for the time range where this expiry was active\n",
    "    start_dt = group['timestamp'].min()\n",
    "    end_dt = group['timestamp'].max()\n",
    "    \n",
    "    for strike in unique_strikes:\n",
    "        ce_symbol = f\"NSE-NIFTY-{expiry}-{strike}-CE\"\n",
    "        pe_symbol = f\"NSE-NIFTY-{expiry}-{strike}-PE\"\n",
    "        \n",
    "        for symbol in [ce_symbol, pe_symbol]:\n",
    "            cache_key = symbol \n",
    "            if cache_key in option_cache: continue\n",
    "            \n",
    "            print(f\"  Fetching {symbol}...\", end=\"\")\n",
    "            \n",
    "            current_start = start_dt\n",
    "            all_candles = []\n",
    "            while current_start < end_dt:\n",
    "                current_end = current_start + timedelta(days=5)\n",
    "                if current_end > end_dt: current_end = end_dt\n",
    "                \n",
    "                try:\n",
    "                    resp = groww.get_historical_candles(\n",
    "                        exchange=\"NSE\",\n",
    "                        segment=\"FNO\",\n",
    "                        groww_symbol=symbol,\n",
    "                        start_time=current_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        end_time=current_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        candle_interval=\"1minute\"\n",
    "                    )\n",
    "                    if resp and \"candles\" in resp:\n",
    "                        all_candles.extend(resp[\"candles\"])\n",
    "                except:\n",
    "                    pass\n",
    "                current_start = current_end\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            if all_candles:\n",
    "                temp_df = pd.DataFrame(all_candles)\n",
    "                if len(temp_df.columns) >= 5:\n",
    "                    # Rename columns 0->ts, 4->close\n",
    "                    rename_map = {0: \"timestamp\", 4: \"close\"}\n",
    "                    temp_df.rename(columns=rename_map, inplace=True)\n",
    "                \n",
    "                # Timestamp cleanup\n",
    "                first_ts = temp_df[\"timestamp\"].iloc[0]\n",
    "                if isinstance(first_ts, str):\n",
    "                    temp_df[\"timestamp\"] = pd.to_datetime(temp_df[\"timestamp\"])\n",
    "                else:\n",
    "                    temp_df[\"timestamp\"] = pd.to_datetime(temp_df[\"timestamp\"], unit='s')\n",
    "                \n",
    "                temp_df.set_index(\"timestamp\", inplace=True)\n",
    "                # Remove duplicates just in case\n",
    "                temp_df = temp_df[~temp_df.index.duplicated(keep='first')]\n",
    "                \n",
    "                option_cache[cache_key] = temp_df\n",
    "                print(\" Done\")\n",
    "            else:\n",
    "                print(\" Failed (No Data)\")\n",
    "\n",
    "# --- STEP 3: STITCH FINAL FILE ---\n",
    "print(f\"\\n--- Stitching {OUTPUT_FILE} ---\")\n",
    "final_rows = []\n",
    "\n",
    "for index, row in df_spot.iterrows():\n",
    "    ts = row['timestamp']\n",
    "    nifty_ltp = row['close']\n",
    "    atm_strike = int(row['atm_strike'])\n",
    "    expiry = row['expiry_str']\n",
    "    \n",
    "    # Construct exact keys\n",
    "    ce_key = f\"NSE-NIFTY-{expiry}-{atm_strike}-CE\"\n",
    "    pe_key = f\"NSE-NIFTY-{expiry}-{atm_strike}-PE\"\n",
    "    \n",
    "    ce_ltp = None\n",
    "    pe_ltp = None\n",
    "    \n",
    "    # Lookup CE\n",
    "    if ce_key in option_cache:\n",
    "        # We need to handle potential missing minutes (ffill logic is usually better, but keeping it simple)\n",
    "        try:\n",
    "            ce_ltp = option_cache[ce_key].loc[ts]['close']\n",
    "        except KeyError:\n",
    "            pass # Data missing for this exact minute\n",
    "            \n",
    "    # Lookup PE\n",
    "    if pe_key in option_cache:\n",
    "        try:\n",
    "            pe_ltp = option_cache[pe_key].loc[ts]['close']\n",
    "        except KeyError:\n",
    "            pass\n",
    "            \n",
    "    final_rows.append({\n",
    "        \"timestamp\": ts,\n",
    "        \"nifty_ltp\": nifty_ltp,\n",
    "        \"expiry\": expiry,\n",
    "        \"atm_strike\": atm_strike,\n",
    "        \"atm_ce_ltp\": ce_ltp,\n",
    "        \"atm_pe_ltp\": pe_ltp\n",
    "    })\n",
    "\n",
    "# Save\n",
    "final_df = pd.DataFrame(final_rows)\n",
    "final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nSUCCESS! Saved {len(final_df)} rows to {OUTPUT_FILE}\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458cc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nifty_spot_1m.csv...\n",
      "Expiries required: ['11Nov25' '18Nov25' '25Nov25' '02Dec25' '09Dec25']\n",
      "Ready to Groww!\n",
      "\n",
      "--- Downloading Contracts ---\n",
      "\n",
      "Processing Expiry: 02Dec25 (Strikes: 14)\n",
      "  Fetching NSE-NIFTY-02Dec25-25600-CE... Done (1408 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25600-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25700-CE... Done (1809 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25700-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25750-CE... Done (1578 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25750-PE... Done (1878 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25800-CE... Done (1873 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25800-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25850-CE... Done (1816 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25850-PE... Done (1878 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25950-CE... Done (1873 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-25950-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26000-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26000-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26050-CE... Done (1877 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26050-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26100-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26100-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26150-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26150-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26200-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26200-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26300-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26300-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26250-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26250-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26350-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-02Dec25-26350-PE... Done (1878 rows)\n",
      "\n",
      "Processing Expiry: 09Dec25 (Strikes: 8)\n",
      "  Fetching NSE-NIFTY-09Dec25-25850-CE... Done (1127 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-25850-PE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26000-CE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26000-PE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-25950-CE... Done (1126 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-25950-PE... Done (1127 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26050-CE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26050-PE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-25900-CE... Done (1127 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-25900-PE... Done (1127 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26100-CE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26100-PE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26150-CE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26150-PE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26200-CE... Done (1128 rows)\n",
      "  Fetching NSE-NIFTY-09Dec25-26200-PE... Done (1128 rows)\n",
      "\n",
      "Processing Expiry: 11Nov25 (Strikes: 6)\n",
      "  Fetching NSE-NIFTY-11Nov25-25450-CE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25450-PE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25500-CE... Done (829 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25500-PE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25550-CE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25550-PE... Done (829 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25600-CE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25600-PE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25650-CE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25650-PE... Done (829 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25700-CE... Done (830 rows)\n",
      "  Fetching NSE-NIFTY-11Nov25-25700-PE... Done (830 rows)\n",
      "\n",
      "Processing Expiry: 18Nov25 (Strikes: 12)\n",
      "  Fetching NSE-NIFTY-18Nov25-25800-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25800-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25850-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25850-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25900-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25900-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25950-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25950-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26000-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26000-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25550-CE... Done (1785 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25550-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25700-CE... Done (1878 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25700-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25650-CE... Done (1873 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25650-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25750-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-25750-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26150-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26150-PE... Done (1877 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26100-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26100-PE... Done (1878 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26050-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-18Nov25-26050-PE... Done (1878 rows)\n",
      "\n",
      "Processing Expiry: 25Nov25 (Strikes: 9)\n",
      "  Fetching NSE-NIFTY-25Nov25-25950-CE... Done (1877 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-25950-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26250-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26250-PE... Done (1878 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26000-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26000-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-25900-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-25900-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-25850-CE... Done (1877 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-25850-PE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26050-CE... Done (1879 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26050-PE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26200-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26200-PE... Done (1878 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26150-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26150-PE... Done (1877 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26100-CE... Done (1880 rows)\n",
      "  Fetching NSE-NIFTY-25Nov25-26100-PE... Done (1878 rows)\n",
      "\n",
      "--- Stitching Final_option_ltp_1m_rolling.csv ---\n",
      "\n",
      "SUCCESS! Saved 8507 rows to Final_option_ltp_1m_rolling.csv\n",
      "            timestamp  nifty_ltp   expiry  atm_strike  \\\n",
      "0 2025-11-07 14:13:00   25473.70  11Nov25       25450   \n",
      "1 2025-11-07 14:14:00   25474.00  11Nov25       25450   \n",
      "2 2025-11-07 14:15:00   25470.55  11Nov25       25450   \n",
      "3 2025-11-07 14:16:00   25479.70  11Nov25       25500   \n",
      "4 2025-11-07 14:17:00   25474.60  11Nov25       25450   \n",
      "\n",
      "                    ce_symbol                   pe_symbol  atm_ce_ltp  \\\n",
      "0  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      109.80   \n",
      "1  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      111.70   \n",
      "2  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      109.60   \n",
      "3  NSE-NIFTY-11Nov25-25500-CE  NSE-NIFTY-11Nov25-25500-PE       85.85   \n",
      "4  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      111.70   \n",
      "\n",
      "   atm_pe_ltp  \n",
      "0       72.70  \n",
      "1       71.55  \n",
      "2       73.40  \n",
      "3       93.50  \n",
      "4       71.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from growwapi import GrowwAPI\n",
    "\n",
    "# --- CREDENTIALS ---\n",
    "# Replace with your actual credentials\n",
    "API_KEY = \"eyJraWQiOiJaTUtjVXciLCJhbGciOiJFUzI1NiJ9.eyJleHAiOjI1NTM0ODUwMTcsImlhdCI6MTc2NTA4NTAxNywibmJmIjoxNzY1MDg1MDE3LCJzdWIiOiJ7XCJ0b2tlblJlZklkXCI6XCJmYjg0YzJmOS04NGUwLTQ2NGMtYWFkZC0wZjMyZTBiNDZmY2FcIixcInZlbmRvckludGVncmF0aW9uS2V5XCI6XCJlMzFmZjIzYjA4NmI0MDZjODg3NGIyZjZkODQ5NTMxM1wiLFwidXNlckFjY291bnRJZFwiOlwiMDdmMDA0MGMtZTk4Zi00ZDNmLTk5Y2EtZDc1ZjBlYWU5M2NlXCIsXCJkZXZpY2VJZFwiOlwiZDMyMWIxMzUtZWQ5Mi01ZWJkLWJjMDUtZTY1NDY2OWRiMDM5XCIsXCJzZXNzaW9uSWRcIjpcIjVlZDUwZmU2LTBiNjktNDBlMC04ZDJmLTJlZjE3Y2YxZDYwN1wiLFwiYWRkaXRpb25hbERhdGFcIjpcIno1NC9NZzltdjE2WXdmb0gvS0EwYk1yOE5XVzhzdTNvZ080am1ZUzIwZEpSTkczdTlLa2pWZDNoWjU1ZStNZERhWXBOVi9UOUxIRmtQejFFQisybTdRPT1cIixcInJvbGVcIjpcImF1dGgtdG90cFwiLFwic291cmNlSXBBZGRyZXNzXCI6XCIyNDA5OjQwOTA6MTA4ZjpkYzA1OjFjNDg6YTliYjo5MTZiOmI4NWQsMTcyLjcxLjE5OC4xMjgsMzUuMjQxLjIzLjEyM1wiLFwidHdvRmFFeHBpcnlUc1wiOjI1NTM0ODUwMTc4ODR9IiwiaXNzIjoiYXBleC1hdXRoLXByb2QtYXBwIn0.GCoXAEdA0BkhB88lQmsYqzl96qaGudoM3UvzHxEh_tGfODPmrLzTNPMo8KCeTpzwf46Hp-wU41QxjNPwGyHmag\"\n",
    "API_SECRET = \"F@ldixy2hTCYKBq30fyNIyz#PaJ1Ui9i\"\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SPOT_FILE = \"Final_nifty_spot_1minute_fixed_trimmed.csv\"\n",
    "OUTPUT_FILE = \"option_ltp.csv\"\n",
    "\n",
    "# --- USER DEFINED EXPIRY SCHEDULE (Tuesdays) ---\n",
    "# Confirmed: Nifty expiry moves to Tuesday in late 2025.\n",
    "EXPIRY_SCHEDULE = [\n",
    "    pd.Timestamp(\"2025-11-04\"),\n",
    "    pd.Timestamp(\"2025-11-11\"),\n",
    "    pd.Timestamp(\"2025-11-18\"),\n",
    "    pd.Timestamp(\"2025-11-25\"), # Monthly Expiry\n",
    "    pd.Timestamp(\"2025-12-02\"),\n",
    "    pd.Timestamp(\"2025-12-09\")\n",
    "]\n",
    "\n",
    "def auth():\n",
    "    try:\n",
    "        token = GrowwAPI.get_access_token(api_key=API_KEY, secret=API_SECRET)\n",
    "        return GrowwAPI(token)\n",
    "    except Exception as e:\n",
    "        print(f\"Auth failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "def get_target_expiry_date(current_ts):\n",
    "    \"\"\"Returns the Timestamp of the next expiry.\"\"\"\n",
    "    current_date = current_ts.date()\n",
    "    for exp_ts in EXPIRY_SCHEDULE:\n",
    "        if exp_ts.date() >= current_date:\n",
    "            return exp_ts\n",
    "    return None\n",
    "\n",
    "# --- STEP 1: PREPARE SPOT DATA ---\n",
    "print(f\"Loading {SPOT_FILE}...\")\n",
    "try:\n",
    "    df_spot = pd.read_csv(SPOT_FILE)\n",
    "    if isinstance(df_spot['timestamp'].iloc[0], str):\n",
    "        df_spot['timestamp'] = pd.to_datetime(df_spot['timestamp'])\n",
    "    else:\n",
    "        df_spot['timestamp'] = pd.to_datetime(df_spot['timestamp'], unit='s')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {SPOT_FILE} not found.\")\n",
    "    exit()\n",
    "\n",
    "# 1. Calculate ATM Strike\n",
    "df_spot['atm_strike'] = (df_spot['close'] / 50).round() * 50\n",
    "\n",
    "# 2. Assign Expiry Date\n",
    "df_spot['expiry_dt'] = df_spot['timestamp'].apply(get_target_expiry_date)\n",
    "df_spot.dropna(subset=['expiry_dt'], inplace=True)\n",
    "\n",
    "# Generate formatted string for grouping (e.g. 25Nov25)\n",
    "df_spot['expiry_str'] = df_spot['expiry_dt'].dt.strftime(\"%d%b%y\")\n",
    "\n",
    "print(\"Expiries required:\", df_spot['expiry_str'].unique())\n",
    "\n",
    "# --- STEP 2: DOWNLOAD BATCH HISTORY ---\n",
    "groww = auth()\n",
    "option_cache = {} \n",
    "grouped = df_spot.groupby('expiry_str')\n",
    "\n",
    "print(\"\\n--- Downloading Contracts ---\")\n",
    "\n",
    "for expiry_str, group in grouped:\n",
    "    unique_strikes = group['atm_strike'].unique().astype(int)\n",
    "    print(f\"\\nProcessing Expiry: {expiry_str} (Strikes: {len(unique_strikes)})\")\n",
    "    \n",
    "    start_dt = group['timestamp'].min()\n",
    "    end_dt = group['timestamp'].max()\n",
    "    \n",
    "    # Identify if this might be monthly (e.g. Nov 25) to prepare alternate format\n",
    "    # Alternate format: MONYY (e.g. Nov25)\n",
    "    # We get this from the timestamp object we stored\n",
    "    sample_ts = group['expiry_dt'].iloc[0]\n",
    "    alt_expiry_str = sample_ts.strftime(\"%b%y\") # \"Nov25\"\n",
    "    \n",
    "    for strike in unique_strikes:\n",
    "        # We will try two formats: Weekly (25Nov25) and Monthly (Nov25)\n",
    "        # 1. Standard Weekly Format\n",
    "        ce_weekly = f\"NSE-NIFTY-{expiry_str}-{strike}-CE\"\n",
    "        pe_weekly = f\"NSE-NIFTY-{expiry_str}-{strike}-PE\"\n",
    "        \n",
    "        # 2. Monthly Format (Fallback)\n",
    "        ce_monthly = f\"NSE-NIFTY-{alt_expiry_str}-{strike}-CE\"\n",
    "        pe_monthly = f\"NSE-NIFTY-{alt_expiry_str}-{strike}-PE\"\n",
    "        \n",
    "        # Pairs to try: (Standard, Fallback)\n",
    "        pairs = [(ce_weekly, ce_monthly), (pe_weekly, pe_monthly)]\n",
    "        \n",
    "        for std_sym, alt_sym in pairs:\n",
    "            # Check if we already have either\n",
    "            if std_sym in option_cache or alt_sym in option_cache:\n",
    "                continue\n",
    "            \n",
    "            # TRY STANDARD\n",
    "            target_sym = std_sym\n",
    "            print(f\"  Fetching {target_sym}...\", end=\"\")\n",
    "            \n",
    "            candles = []\n",
    "            success = False\n",
    "            \n",
    "            # Function to fetch data\n",
    "            def fetch_data(sym):\n",
    "                c = []\n",
    "                curr = start_dt\n",
    "                while curr < end_dt:\n",
    "                    nxt = curr + timedelta(days=5)\n",
    "                    if nxt > end_dt: nxt = end_dt\n",
    "                    try:\n",
    "                        resp = groww.get_historical_candles(\n",
    "                            exchange=\"NSE\", segment=\"FNO\", groww_symbol=sym,\n",
    "                            start_time=curr.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                            end_time=nxt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                            candle_interval=\"1minute\"\n",
    "                        )\n",
    "                        if resp and \"candles\" in resp: c.extend(resp[\"candles\"])\n",
    "                    except Exception as e:\n",
    "                        pass # Ignore chunk errors\n",
    "                    curr = nxt\n",
    "                    time.sleep(0.1)\n",
    "                return c\n",
    "\n",
    "            # Attempt 1: Standard\n",
    "            candles = fetch_data(std_sym)\n",
    "            \n",
    "            if not candles:\n",
    "                # Attempt 2: Fallback (Monthly Format)\n",
    "                print(f\" Failed. Retrying as {alt_sym}...\", end=\"\")\n",
    "                candles = fetch_data(alt_sym)\n",
    "                if candles:\n",
    "                    target_sym = alt_sym # Switch to the working symbol\n",
    "            \n",
    "            if candles:\n",
    "                # Process DataFrame\n",
    "                temp_df = pd.DataFrame(candles)\n",
    "                if len(temp_df.columns) >= 5:\n",
    "                    rename_map = {0: \"timestamp\", 4: \"close\"}\n",
    "                    temp_df.rename(columns=rename_map, inplace=True)\n",
    "                \n",
    "                if isinstance(temp_df[\"timestamp\"].iloc[0], str):\n",
    "                    temp_df[\"timestamp\"] = pd.to_datetime(temp_df[\"timestamp\"])\n",
    "                else:\n",
    "                    temp_df[\"timestamp\"] = pd.to_datetime(temp_df[\"timestamp\"], unit='s')\n",
    "                \n",
    "                temp_df.set_index(\"timestamp\", inplace=True)\n",
    "                temp_df = temp_df[~temp_df.index.duplicated(keep='first')]\n",
    "                \n",
    "                option_cache[target_sym] = temp_df\n",
    "                print(f\" Done ({len(temp_df)} rows)\")\n",
    "            else:\n",
    "                print(\" Failed (No Data)\")\n",
    "\n",
    "# --- STEP 3: STITCH FINAL FILE ---\n",
    "print(f\"\\n--- Stitching {OUTPUT_FILE} ---\")\n",
    "final_rows = []\n",
    "\n",
    "for index, row in df_spot.iterrows():\n",
    "    ts = row['timestamp']\n",
    "    nifty_ltp = row['close']\n",
    "    atm_strike = int(row['atm_strike'])\n",
    "    expiry_str = row['expiry_str']\n",
    "    \n",
    "    # Reconstruct keys to look up\n",
    "    # Must match logic above (Try Standard, then Fallback)\n",
    "    \n",
    "    # Helper to find which symbol exists in cache\n",
    "    def get_data(std, alt):\n",
    "        if std in option_cache: return std, option_cache[std]\n",
    "        if alt in option_cache: return alt, option_cache[alt]\n",
    "        return None, None\n",
    "\n",
    "    sample_ts = row['expiry_dt']\n",
    "    alt_expiry_str = sample_ts.strftime(\"%b%y\") # \"Nov25\"\n",
    "    \n",
    "    ce_std = f\"NSE-NIFTY-{expiry_str}-{atm_strike}-CE\"\n",
    "    ce_alt = f\"NSE-NIFTY-{alt_expiry_str}-{atm_strike}-CE\"\n",
    "    \n",
    "    pe_std = f\"NSE-NIFTY-{expiry_str}-{atm_strike}-PE\"\n",
    "    pe_alt = f\"NSE-NIFTY-{alt_expiry_str}-{atm_strike}-PE\"\n",
    "    \n",
    "    # CE Lookup\n",
    "    ce_used, ce_df = get_data(ce_std, ce_alt)\n",
    "    ce_ltp = ce_df.loc[ts]['close'] if (ce_df is not None and ts in ce_df.index) else None\n",
    "    \n",
    "    # PE Lookup\n",
    "    pe_used, pe_df = get_data(pe_std, pe_alt)\n",
    "    pe_ltp = pe_df.loc[ts]['close'] if (pe_df is not None and ts in pe_df.index) else None\n",
    "    \n",
    "    final_rows.append({\n",
    "        \"timestamp\": ts,\n",
    "        \"nifty_ltp\": nifty_ltp,\n",
    "        \"expiry\": expiry_str,\n",
    "        \"atm_strike\": atm_strike,\n",
    "        \"ce_symbol\": ce_used if ce_used else ce_std, # Log the symbol used\n",
    "        \"pe_symbol\": pe_used if pe_used else pe_std,\n",
    "        \"atm_ce_ltp\": ce_ltp,\n",
    "        \"atm_pe_ltp\": pe_ltp\n",
    "    })  \n",
    "\n",
    "# Save\n",
    "final_df = pd.DataFrame(final_rows)\n",
    "final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nSUCCESS! Saved {len(final_df)} rows to {OUTPUT_FILE}\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cffd5c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison of 'atm_pe_ltp' ===\n",
      "Mean Difference: 0.00\n",
      "Max Difference: 0.00\n",
      "Min Difference: 0.00\n",
      "Std Dev: 0.00\n",
      "\n",
      "Top 10 Largest Differences:\n",
      "                     atm_pe_ltp                    atm_pe_ltp  atm_pe_ltp_diff\n",
      "0 1970-01-01 00:00:00.000000072 1970-01-01 00:00:00.000000072              0.0\n",
      "1 1970-01-01 00:00:00.000000071 1970-01-01 00:00:00.000000071              0.0\n",
      "2 1970-01-01 00:00:00.000000073 1970-01-01 00:00:00.000000073              0.0\n",
      "3 1970-01-01 00:00:00.000000093 1970-01-01 00:00:00.000000093              0.0\n",
      "4 1970-01-01 00:00:00.000000071 1970-01-01 00:00:00.000000071              0.0\n",
      "5 1970-01-01 00:00:00.000000090 1970-01-01 00:00:00.000000090              0.0\n",
      "6 1970-01-01 00:00:00.000000088 1970-01-01 00:00:00.000000088              0.0\n",
      "7 1970-01-01 00:00:00.000000086 1970-01-01 00:00:00.000000086              0.0\n",
      "8 1970-01-01 00:00:00.000000085 1970-01-01 00:00:00.000000085              0.0\n",
      "9 1970-01-01 00:00:00.000000086 1970-01-01 00:00:00.000000086              0.0\n",
      "\n",
      "Comparison saved to 'comparison_result.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load and compare two different CSV files by the same column\n",
    "\n",
    "# 1. Load both CSV files\n",
    "df1 = pd.read_csv('option_ltp_1m_rolling.csv')  # Replace with your first CSV file\n",
    "df2 = pd.read_csv('Final_option_ltp_1m_rolling.csv')  # Replace with your second CSV file\n",
    "\n",
    "# 2. Compare a specific column (e.g., 'atm_ce_ltp')\n",
    "column_to_compare = 'atm_pe_ltp'  # Change this to your column name\n",
    "\n",
    "# Convert to datetime if comparing timestamps\n",
    "if column_to_compare == 'atm_pe_ltp':\n",
    "\tdf1['atm_pe_ltp'] = pd.to_datetime(df1['atm_pe_ltp'])\n",
    "\tdf2['atm_pe_ltp'] = pd.to_datetime(df2['atm_pe_ltp'])\n",
    "\t# Calculate time difference in seconds\n",
    "\tdf1[f'{column_to_compare}_diff'] = (df1['atm_pe_ltp'] - df2['atm_pe_ltp']).dt.total_seconds()\n",
    "else:\n",
    "\t# Calculate differences for numeric columns\n",
    "\tdf1[f'{column_to_compare}_diff'] = df1[column_to_compare] - df2[column_to_compare]\n",
    "\n",
    "# 3. Summary statistics\n",
    "print(f\"=== Comparison of '{column_to_compare}' ===\")\n",
    "print(f\"Mean Difference: {df1[f'{column_to_compare}_diff'].mean():.2f}\")\n",
    "print(f\"Max Difference: {df1[f'{column_to_compare}_diff'].max():.2f}\")\n",
    "print(f\"Min Difference: {df1[f'{column_to_compare}_diff'].min():.2f}\")\n",
    "print(f\"Std Dev: {df1[f'{column_to_compare}_diff'].std():.2f}\")\n",
    "\n",
    "# 4. View rows with largest differences\n",
    "print(f\"\\nTop 10 Largest Differences:\")\n",
    "print(df1.nlargest(10, f'{column_to_compare}_diff')[['atm_pe_ltp', column_to_compare, f'{column_to_compare}_diff']])\n",
    "\n",
    "# 5. Save comparison\n",
    "df1[['timestamp', column_to_compare, f'{column_to_compare}_diff']].to_csv('comparison_result.csv', index=False)\n",
    "print(\"\\nComparison saved to 'comparison_result.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feb60920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Final_option_ltp_1m_rolling.csv...\n",
      "Computing Future Targets...\n",
      "Computing Lags & Momentum...\n",
      "Computing Time Features...\n",
      "Finalizing...\n",
      "------------------------------\n",
      " SUCCESS: Saved to option_ltp_features_1m.csv\n",
      "Total Rows: 7387\n",
      "Proportion CE UP (1m): 48.57%\n",
      "Proportion PE UP (1m): 49.05%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"Final_option_ltp_1m_rolling.csv\"  # Or \"Final_option_ltp_1m_rolling.csv\"\n",
    "OUTPUT_FILE = \"option_ltp_features_1m.csv\"\n",
    "\n",
    "def prepare_features():\n",
    "    print(f\"Reading {INPUT_FILE}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        # Try alternative name\n",
    "        df = pd.read_csv(\"Final_option_ltp_1m_rolling.csv\")\n",
    "\n",
    "    # 1. Basic Cleanup\n",
    "    # Ensure timestamp is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sort by time to ensure shifts work correctly\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Robustness: Reconstruct Symbols if missing\n",
    "    # (Format: NSE-NIFTY-{expiry}-{strike}-CE)\n",
    "    if 'ce_sym' not in df.columns:\n",
    "        df['ce_sym'] = 'NSE-NIFTY-' + df['expiry'].astype(str) + '-' + df['atm_strike'].astype(str) + '-CE'\n",
    "    if 'pe_sym' not in df.columns:\n",
    "        df['pe_sym'] = 'NSE-NIFTY-' + df['expiry'].astype(str) + '-' + df['atm_strike'].astype(str) + '-PE'\n",
    "\n",
    "    # --- PART 1: FUTURE RETURNS (TARGETS) ---\n",
    "    print(\"Computing Future Targets...\")\n",
    "    for k in [1, 3, 5]:\n",
    "        # Shift BACKWARDS to see into the future (t+k)\n",
    "        nifty_future = df['nifty_ltp'].shift(-k)\n",
    "        ce_future = df['atm_ce_ltp'].shift(-k)\n",
    "        pe_future = df['atm_pe_ltp'].shift(-k)\n",
    "        \n",
    "        # Verify time continuity (Ensure we don't jump days)\n",
    "        time_future = df['timestamp'].shift(-k)\n",
    "        time_diff = (time_future - df['timestamp']).dt.total_seconds() / 60\n",
    "        valid_mask = (np.abs(time_diff - k) < 0.1) # Tolerance for float math\n",
    "        \n",
    "        # Calculate Returns: (Future / Current) - 1\n",
    "        df[f'nifty_ret_{k}m'] = np.where(valid_mask, (nifty_future / df['nifty_ltp']) - 1, np.nan)\n",
    "        df[f'ce_ret_{k}m'] = np.where(valid_mask, (ce_future / df['atm_ce_ltp']) - 1, np.nan)\n",
    "        df[f'pe_ret_{k}m'] = np.where(valid_mask, (pe_future / df['atm_pe_ltp']) - 1, np.nan)\n",
    "\n",
    "    # Binary Direction Labels (1 if > 0, else 0)\n",
    "    df['ce_up_1m'] = (df['ce_ret_1m'] > 0).astype(int)\n",
    "    df['pe_up_1m'] = (df['pe_ret_1m'] > 0).astype(int)\n",
    "\n",
    "    # --- PART 2: LAGS & MOMENTUM (PAST FEATURES) ---\n",
    "    print(\"Computing Lags & Momentum...\")\n",
    "    \n",
    "    # Check time continuity for past lag (t-1)\n",
    "    time_prev = df['timestamp'].shift(1)\n",
    "    time_diff_prev = (df['timestamp'] - time_prev).dt.total_seconds() / 60\n",
    "    valid_lag = (np.abs(time_diff_prev - 1) < 0.1)\n",
    "\n",
    "    # Lag 1 Returns: (Current / Previous) - 1\n",
    "    # Note: We calculate this manually to handle the valid_lag mask properly\n",
    "    nifty_ret_past = (df['nifty_ltp'] / df['nifty_ltp'].shift(1)) - 1\n",
    "    nifty_ret_past[~valid_lag] = np.nan # Mask invalid lags (e.g. first minute of day)\n",
    "\n",
    "    ce_ret_past = (df['atm_ce_ltp'] / df['atm_ce_ltp'].shift(1)) - 1\n",
    "    ce_ret_past[~valid_lag] = np.nan\n",
    "\n",
    "    pe_ret_past = (df['atm_pe_ltp'] / df['atm_pe_ltp'].shift(1)) - 1\n",
    "    pe_ret_past[~valid_lag] = np.nan\n",
    "\n",
    "    # Assign Lags\n",
    "    df['nifty_ret_1m_lag1'] = nifty_ret_past\n",
    "    df['ce_ret_1m_lag1'] = ce_ret_past\n",
    "    df['pe_ret_1m_lag1'] = pe_ret_past\n",
    "\n",
    "    # Rolling Features (Using the past returns calculated above)\n",
    "    # Rolling Std of 1-min returns (Window=3)\n",
    "    df['nifty_ret_3m_rolling_std'] = nifty_ret_past.rolling(window=3).std()\n",
    "    \n",
    "    # Rolling Mean of Price (Window=5)\n",
    "    df['nifty_ma_5m'] = df['nifty_ltp'].rolling(window=5).mean()\n",
    "    \n",
    "    # Rolling Std of Option Returns (Window=5)\n",
    "    df['ce_rolling_std_5m'] = ce_ret_past.rolling(window=5).std()\n",
    "    df['pe_rolling_std_5m'] = pe_ret_past.rolling(window=5).std()\n",
    "\n",
    "    # Spread Features\n",
    "    df['ce_pe_mid'] = (df['atm_ce_ltp'] + df['atm_pe_ltp']) / 2\n",
    "    df['ce_minus_pe'] = df['atm_ce_ltp'] - df['atm_pe_ltp']\n",
    "\n",
    "    # --- PART 3: TIME FEATURES ---\n",
    "    print(\"Computing Time Features...\")\n",
    "    \n",
    "    # Helper: Minutes from 09:15\n",
    "    def get_minutes_from_open(dt):\n",
    "        return (dt.hour * 60 + dt.minute) - (9 * 60 + 15)\n",
    "\n",
    "    df['minute_of_day'] = df['timestamp'].apply(get_minutes_from_open)\n",
    "    \n",
    "    # Opening Hour: 09:15 to 09:45 (0 to 30 mins)\n",
    "    df['is_opening_hour'] = ((df['minute_of_day'] >= 0) & (df['minute_of_day'] <= 30)).astype(int)\n",
    "    \n",
    "    # Closing Hour: After 15:00\n",
    "    df['is_closing_hour'] = (df['timestamp'].dt.time >= pd.to_datetime('15:00:00').time()).astype(int)\n",
    "\n",
    "    # --- PART 4: FINAL CLEANUP & SAVE ---\n",
    "    print(\"Finalizing...\")\n",
    "\n",
    "    # Define Target Columns (Futures)\n",
    "    target_cols = [\n",
    "        'nifty_ret_1m', 'nifty_ret_3m', 'nifty_ret_5m',\n",
    "        'ce_ret_1m', 'ce_ret_3m', 'ce_ret_5m',\n",
    "        'pe_ret_1m', 'pe_ret_3m', 'pe_ret_5m'\n",
    "    ]\n",
    "\n",
    "    # 1. Drop rows where we don't know the future (Targets are NaN)\n",
    "    df_final = df.dropna(subset=target_cols).copy()\n",
    "    \n",
    "    # 2. Drop rows where we don't know the past (Features are NaN, e.g. first 5 mins of day)\n",
    "    # This ensures your AI doesn't crash on NaNs\n",
    "    df_final = df_final.dropna()\n",
    "\n",
    "    # Define Column Order\n",
    "    base_cols = ['timestamp', 'expiry', 'atm_strike', 'ce_sym', 'pe_sym', 'nifty_ltp', 'atm_ce_ltp', 'atm_pe_ltp']\n",
    "    \n",
    "    feature_cols = [\n",
    "        'nifty_ret_1m_lag1', 'nifty_ret_3m_rolling_std', 'nifty_ma_5m',\n",
    "        'ce_ret_1m_lag1', 'pe_ret_1m_lag1',\n",
    "        'ce_rolling_std_5m', 'pe_rolling_std_5m',\n",
    "        'ce_pe_mid', 'ce_minus_pe',\n",
    "        'minute_of_day', 'is_opening_hour', 'is_closing_hour'\n",
    "    ]\n",
    "    \n",
    "    binary_targets = ['ce_up_1m', 'pe_up_1m']\n",
    "    \n",
    "    # Combine all\n",
    "    final_order = base_cols + feature_cols + target_cols + binary_targets\n",
    "    df_final = df_final[final_order]\n",
    "\n",
    "    # Save\n",
    "    df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    # Print Stats\n",
    "    print(\"-\" * 30)\n",
    "    print(f\" SUCCESS: Saved to {OUTPUT_FILE}\")\n",
    "    print(f\"Total Rows: {len(df_final)}\")\n",
    "    print(f\"Proportion CE UP (1m): {df_final['ce_up_1m'].mean():.2%}\")\n",
    "    print(f\"Proportion PE UP (1m): {df_final['pe_up_1m'].mean():.2%}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a17a3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:75: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:82: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:83: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_2988\\3633792695.py:92: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_features = df.groupby('market_date', group_keys=False).apply(compute_group_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\StockMarket\\StockMarket\\notebooks\\F_option_ltp_features_1m.csv\n",
      "Rows: 7576\n",
      "CE up 1m: 3679/7576 = 0.486\n",
      "PE up 1m: 3699/7576 = 0.488\n",
      "\n",
      "Sample rows (first 8):\n",
      "                timestamp  expiry  atm_strike                  ce_symbol                  pe_symbol  nifty_ltp  atm_ce_ltp  atm_pe_ltp  nifty_ret_1m  ce_ret_1m  pe_ret_1m  nifty_ret_3m  ce_ret_3m  pe_ret_3m  nifty_ret_5m  ce_ret_5m  pe_ret_5m  ce_up_1m  pe_up_1m  nifty_ret_1m_lag1  ce_ret_1m_lag1  pe_ret_1m_lag1  nifty_ret_3m_rolling_std  nifty_ma_5m  ce_rolling_std_5m  pe_rolling_std_5m  ce_pe_mid  ce_minus_pe  minute_of_day  is_opening_hour  is_closing_hour\n",
      "2025-11-07 14:13:00+05:30 11Nov25       25450 NSE-NIFTY-11Nov25-25450-CE NSE-NIFTY-11Nov25-25450-PE   25473.70      109.80       72.70      0.000012   0.017304  -0.015818      0.000236  -0.218124   0.286107      0.000120  -0.201730   0.248281         1         0                NaN             NaN             NaN                       NaN   25473.7000                NaN                NaN     91.250        37.10            298                0                0\n",
      "2025-11-07 14:14:00+05:30 11Nov25       25450 NSE-NIFTY-11Nov25-25450-CE NSE-NIFTY-11Nov25-25450-PE   25474.00      111.70       71.55     -0.000135  -0.018800   0.025856      0.000024   0.000000  -0.007687      0.000322  -0.203671   0.241090         0         1           0.000012        0.017304       -0.015818                       NaN   25473.8500                NaN                NaN     91.625        40.15            299                0                0\n",
      "2025-11-07 14:15:00+05:30 11Nov25       25450 NSE-NIFTY-11Nov25-25450-CE NSE-NIFTY-11Nov25-25450-PE   25470.55      109.60       73.40      0.000359  -0.216697   0.273842      0.000243  -0.200274   0.236376      0.000744  -0.166971   0.173025         0         1          -0.000135       -0.018800        0.025856                  0.000104   25472.7500           0.025530           0.029468     91.500        36.20            300                0                0\n",
      "2025-11-07 14:16:00+05:30 11Nov25       25500 NSE-NIFTY-11Nov25-25500-CE NSE-NIFTY-11Nov25-25500-PE   25479.70       85.85       93.50     -0.000200   0.301107  -0.240642      0.000098   0.036109  -0.050267      0.000404   0.059406  -0.081283         1         0           0.000359       -0.216697        0.273842                  0.000254   25474.4875           0.125978           0.156598     89.675        -7.65            301                0                0\n",
      "2025-11-07 14:17:00+05:30 11Nov25       25450 NSE-NIFTY-11Nov25-25450-CE NSE-NIFTY-11Nov25-25450-PE   25474.60      111.70       71.00      0.000084  -0.215309   0.278169      0.000585  -0.182632   0.212676      0.000620  -0.176365   0.211268         0         1          -0.000200        0.301107       -0.240642                  0.000306   25474.5100           0.213352           0.210831     91.350        40.70            302                0                0\n",
      "2025-11-07 14:18:00+05:30 11Nov25       25500 NSE-NIFTY-11Nov25-25500-CE NSE-NIFTY-11Nov25-25500-PE   25476.75       87.65       90.75      0.000214   0.014832  -0.021488      0.000520   0.037650  -0.053444      0.001403   0.248146  -0.193388         1         0           0.000084       -0.215309        0.278169                  0.000280   25475.1200           0.212796           0.218251     89.200        -3.10            303                0                0\n",
      "2025-11-07 14:19:00+05:30 11Nov25       25500 NSE-NIFTY-11Nov25-25500-CE NSE-NIFTY-11Nov25-25500-PE   25482.20       88.95       88.80      0.000286   0.026419  -0.030405      0.000322   0.034289  -0.031532      0.001334   0.225970  -0.185811         1         0           0.000214        0.014832       -0.021488                  0.000212   25476.7600           0.212671           0.218785     88.875         0.15            304                0                0\n",
      "2025-11-07 14:20:00+05:30 11Nov25       25500 NSE-NIFTY-11Nov25-25500-CE NSE-NIFTY-11Nov25-25500-PE   25489.50       91.30       86.10      0.000020  -0.003834  -0.002323      0.000902   0.198248  -0.149826      0.001040   0.176342  -0.156214         0         0           0.000286        0.026419       -0.030405                  0.000102   25480.5500           0.214063           0.222596     88.700         5.20            305                0                0\n"
     ]
    }
   ],
   "source": [
    "# build_features_from_final.py\n",
    "\"\"\"\n",
    "Reads:  Final_option_ltp_1m_rolling.csv\n",
    "Writes: /mnt/data/option_ltp_features_1m.csv (or ./option_ltp_features_1m.csv if /mnt/data not present)\n",
    "\n",
    "Expected columns anywhere in the file (any order):\n",
    "  timestamp, nifty_ltp, atm_ce_ltp, atm_pe_ltp, expiry, atm_strike, ce_symbol, pe_symbol\n",
    "\n",
    "Produces forward returns (1/3/5m), binary labels, lag features, rolling stats, time features.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from dateutil import tz\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "IN_FILE = Path(\"Final_option_ltp_1m_rolling.csv\")      # input (update path if needed)\n",
    "OUT_FILE = Path(\"F_option_ltp_features_1m.csv\")\n",
    "\n",
    "if not IN_FILE.exists():\n",
    "    print(f\"Input file not found: {IN_FILE.resolve()}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Read CSV (don't force dtype for all columns to preserve order flexibility)\n",
    "df = pd.read_csv(IN_FILE)\n",
    "\n",
    "# Check presence of required columns (names you confirmed)\n",
    "required = {'timestamp','nifty_ltp','atm_ce_ltp','atm_pe_ltp','expiry','atm_strike','ce_symbol','pe_symbol'}\n",
    "missing = required - set(df.columns.str.strip())\n",
    "if missing:\n",
    "    print(\"Missing required columns in input CSV:\", missing)\n",
    "    print(\"Input columns:\", list(df.columns))\n",
    "    sys.exit(1)\n",
    "\n",
    "# Standardize column names (strip spaces)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Parse timestamp and localize/convert to Asia/Kolkata\n",
    "KZ = tz.gettz(\"Asia/Kolkata\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "if df['timestamp'].isna().any():\n",
    "    print(\"Warning: some timestamps could not be parsed. Rows with NaT will be dropped.\")\n",
    "df = df.dropna(subset=['timestamp']).copy()\n",
    "\n",
    "# If naive datetimes, localize to Asia/Kolkata (assume IST)\n",
    "if df['timestamp'].dt.tz is None:\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize(KZ)\n",
    "else:\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_convert(KZ)\n",
    "\n",
    "# Ensure numeric price columns\n",
    "for col in ['nifty_ltp','atm_ce_ltp','atm_pe_ltp','atm_strike']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Sort by timestamp\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Add market_date (IST date) for grouping by trading day\n",
    "df['market_date'] = df['timestamp'].dt.tz_convert(KZ).dt.date\n",
    "\n",
    "# Function to compute features for each market day\n",
    "def compute_group_features(g):\n",
    "    g = g.sort_values('timestamp').copy()\n",
    "    # forward returns: 1,3,5 minutes\n",
    "    for m in (1,3,5):\n",
    "        g[f'nifty_ret_{m}m'] = (g['nifty_ltp'].shift(-m) / g['nifty_ltp']) - 1\n",
    "        g[f'ce_ret_{m}m']    = (g['atm_ce_ltp'].shift(-m) / g['atm_ce_ltp']) - 1\n",
    "        g[f'pe_ret_{m}m']    = (g['atm_pe_ltp'].shift(-m) / g['atm_pe_ltp']) - 1\n",
    "\n",
    "    # binary 1-minute up targets\n",
    "    g['ce_up_1m'] = (g['ce_ret_1m'] > 0).astype(int)\n",
    "    g['pe_up_1m'] = (g['pe_ret_1m'] > 0).astype(int)\n",
    "\n",
    "    # lag returns\n",
    "    g['nifty_ret_1m_lag1'] = g['nifty_ltp'].pct_change(periods=1)\n",
    "    g['ce_ret_1m_lag1'] = g['atm_ce_ltp'].pct_change(periods=1)\n",
    "    g['pe_ret_1m_lag1'] = g['atm_pe_ltp'].pct_change(periods=1)\n",
    "\n",
    "    # rolling stats and moving averages\n",
    "    g['nifty_ret_3m_rolling_std'] = g['nifty_ltp'].pct_change().rolling(window=3, min_periods=1).std()\n",
    "    g['nifty_ma_5m'] = g['nifty_ltp'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "    g['ce_rolling_std_5m'] = g['atm_ce_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
    "    g['pe_rolling_std_5m'] = g['atm_pe_ltp'].pct_change().rolling(window=5, min_periods=1).std()\n",
    "\n",
    "    # straddle mid, skew\n",
    "    g['ce_pe_mid'] = (g['atm_ce_ltp'] + g['atm_pe_ltp']) / 2.0\n",
    "    g['ce_minus_pe'] = g['atm_ce_ltp'] - g['atm_pe_ltp']\n",
    "\n",
    "    return g\n",
    "\n",
    "# Apply per market day\n",
    "df_features = df.groupby('market_date', group_keys=False).apply(compute_group_features)\n",
    "\n",
    "# Time-based features (minute-of-day relative to market open 09:15 IST)\n",
    "df_features['ts_local'] = df_features['timestamp'].dt.tz_convert(KZ)\n",
    "market_open = pd.to_datetime(df_features['ts_local'].dt.date.astype(str) + ' 09:15:00').dt.tz_localize(KZ)\n",
    "df_features['minute_of_day'] = ((df_features['ts_local'] - market_open).dt.total_seconds() / 60).astype('Int64')\n",
    "df_features['is_opening_hour'] = ((df_features['ts_local'].dt.time >= pd.to_datetime('09:15:00').time()) & (df_features['ts_local'].dt.time < pd.to_datetime('09:45:00').time())).astype(int)\n",
    "df_features['is_closing_hour'] = (df_features['ts_local'].dt.time >= pd.to_datetime('15:00:00').time()).astype(int)\n",
    "\n",
    "# Select final columns order: keep basic identity columns first (in your preferred order)\n",
    "ident_cols = ['timestamp','expiry','atm_strike','ce_symbol','pe_symbol','nifty_ltp','atm_ce_ltp','atm_pe_ltp']\n",
    "# Ensure identity columns exist (they should)\n",
    "ident_cols = [c for c in ident_cols if c in df_features.columns]\n",
    "\n",
    "# Feature / target columns: everything else except helper cols\n",
    "drop_cols = {'market_date','ts_local'}\n",
    "all_cols = [c for c in df_features.columns if c not in drop_cols]\n",
    "# Put ident_cols first, rest afterwards\n",
    "final_cols = ident_cols + [c for c in all_cols if c not in ident_cols]\n",
    "\n",
    "final = df_features[final_cols].copy()\n",
    "\n",
    "# Drop rows missing forward 1-min returns (end-of-day)\n",
    "final = final.dropna(subset=['ce_ret_1m','pe_ret_1m','nifty_ret_1m']).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "final.to_csv(OUT_FILE, index=False)\n",
    "print(\"Saved:\", OUT_FILE.resolve())\n",
    "print(\"Rows:\", len(final))\n",
    "\n",
    "# Print CE/PE up proportions (if present)\n",
    "if 'ce_up_1m' in final.columns:\n",
    "    ce_pos = int(final['ce_up_1m'].sum()); ce_total = len(final)\n",
    "    print(f\"CE up 1m: {ce_pos}/{ce_total} = {ce_pos/ce_total:.3f}\")\n",
    "if 'pe_up_1m' in final.columns:\n",
    "    pe_pos = int(final['pe_up_1m'].sum()); pe_total = len(final)\n",
    "    print(f\"PE up 1m: {pe_pos}/{pe_total} = {pe_pos/pe_total:.3f}\")\n",
    "\n",
    "# Quick sample preview\n",
    "print(\"\\nSample rows (first 8):\")\n",
    "print(final.head(8).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec422bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as merged_data.csv\n",
      "            timestamp      open      high       low     close  volume  \\\n",
      "0 2025-11-07 14:13:00  25472.70  25475.05  25471.65  25473.70     NaN   \n",
      "1 2025-11-07 14:14:00  25474.35  25476.35  25472.60  25474.00     NaN   \n",
      "2 2025-11-07 14:15:00  25474.95  25480.40  25469.30  25470.55     NaN   \n",
      "3 2025-11-07 14:16:00  25470.45  25481.80  25470.45  25479.70     NaN   \n",
      "4 2025-11-07 14:17:00  25480.20  25483.45  25473.65  25474.60     NaN   \n",
      "\n",
      "   open_interest  nifty_ltp   expiry  atm_strike                   ce_symbol  \\\n",
      "0            NaN   25473.70  11Nov25       25450  NSE-NIFTY-11Nov25-25450-CE   \n",
      "1            NaN   25474.00  11Nov25       25450  NSE-NIFTY-11Nov25-25450-CE   \n",
      "2            NaN   25470.55  11Nov25       25450  NSE-NIFTY-11Nov25-25450-CE   \n",
      "3            NaN   25479.70  11Nov25       25500  NSE-NIFTY-11Nov25-25500-CE   \n",
      "4            NaN   25474.60  11Nov25       25450  NSE-NIFTY-11Nov25-25450-CE   \n",
      "\n",
      "                    pe_symbol  atm_ce_ltp  atm_pe_ltp  \n",
      "0  NSE-NIFTY-11Nov25-25450-PE      109.80       72.70  \n",
      "1  NSE-NIFTY-11Nov25-25450-PE      111.70       71.55  \n",
      "2  NSE-NIFTY-11Nov25-25450-PE      109.60       73.40  \n",
      "3  NSE-NIFTY-11Nov25-25500-PE       85.85       93.50  \n",
      "4  NSE-NIFTY-11Nov25-25450-PE      111.70       71.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------- INPUT FILES ----------------------\n",
    "option_file = \"Final_option_ltp_1m_rolling.csv\"\n",
    "spot_file = \"nifty_spot_1m.csv\"\n",
    "\n",
    "# ---------------------- LOAD FILES -----------------------\n",
    "df_opt = pd.read_csv(option_file)\n",
    "df_spot = pd.read_csv(spot_file)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df_opt['timestamp'] = pd.to_datetime(df_opt['timestamp'])\n",
    "df_spot['timestamp'] = pd.to_datetime(df_spot['timestamp'])\n",
    "\n",
    "# Sort both files by timestamp\n",
    "df_opt = df_opt.sort_values('timestamp')\n",
    "df_spot = df_spot.sort_values('timestamp')\n",
    "\n",
    "# ---------------------- MERGE ON TIMESTAMP ----------------------\n",
    "# We merge using outer join to keep all timestamps from both datasets\n",
    "# then forward fill option values for candles without option tick\n",
    "df_merged = pd.merge_asof(\n",
    "    df_spot,\n",
    "    df_opt,\n",
    "    on='timestamp',\n",
    "    direction='nearest',      # take nearest option tick to candle timestamp\n",
    "    tolerance=pd.Timedelta('1min')   # option data must be within 1 min range\n",
    ")\n",
    "\n",
    "# If no matching option tick happens within 1 minute  forward fill\n",
    "df_merged = df_merged.ffill()\n",
    "\n",
    "# ---------------------- SAVE RESULT ----------------------\n",
    "df_merged.to_csv(\"merged_data.csv\", index=False)\n",
    "\n",
    "print(\"Merged file saved as merged_data.csv\")\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e568a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\3164520046.py:88: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  start = start.floor('T')\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\3164520046.py:89: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  end = end.ceil('T')\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\3164520046.py:91: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=start, end=end, freq='1T')  # 1-minute freq\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\3164520046.py:155: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  high = merged['high'].astype(float).fillna(method='ffill')\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\3164520046.py:156: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  low  = merged['low'].astype(float).fillna(method='ffill')\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\3164520046.py:157: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  prev_close = merged['close'].shift(1).fillna(method='ffill')\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\3164520046.py:193: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged['vwap'] = (merged['cum_typ_p'] / merged['cum_cnt']).fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged features to merged_features_final.csv\n",
      "Rows: 40417\n",
      "Columns: ['open', 'high', 'low', 'close', 'nifty_ltp', 'atm_strike', 'ce_symbol', 'pe_symbol', 'atm_ce_ltp', 'atm_pe_ltp', 'nifty_ret_1', 'ce_ret_1', 'pe_ret_1', 'mom_3', 'mom_5', 'mom_10', 'vol_short', 'vol_long', 'vol_spike_ratio', 'atr_14', 'breakout_up_20', 'breakout_down_20', 'range_contraction', 'rsi_14', 'macd', 'macd_signal', 'macd_hist', 'vwap', 'vwap_dist', 'vwap_slope', 'ce_pe_ratio', 'ce_minus_pe', 'ce_pe_delta', 'ce_volatility', 'pe_volatility', 'avg_liq_20', 'liquidity_proxy', 'nifty_ema_50', 'nifty_ema_200', 'trend_up', 'trend_down', 'momentum_ratio', 'volatility_weighted_mom', 'z_mom_3', 'z_vol_short', 'z_ce_pe_ratio', 'target_up_1m', 'target_up_2m', 'target_up_3m', 'target_down_1m', 'target_down_2m', 'target_down_3m', 'target_1m_direction']\n"
     ]
    }
   ],
   "source": [
    "# build_merged_features.py\n",
    "# Usage: python build_merged_features.py\n",
    "# Inputs (expected in same folder):\n",
    "#   - option_ltp.csv  (cols: timestamp, nifty_ltp, expiry, atm_strike, ce_symbol, pe_symbol, atm_ce_ltp, atm_pe_ltp)\n",
    "#   - nifty_spot.csv  (cols: timestamp, open, high, low, close)\n",
    "# Output:\n",
    "#   - merged_features_final.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# --------------- CONFIG ---------------\n",
    "OPTION_FILE = \"Final_option_ltp_1m_rolling.csv\"\n",
    "SPOT_FILE   = \"nifty_spot_1m.csv\"\n",
    "OUTPUT_FILE = \"merged_features_final.csv\"\n",
    "\n",
    "# feature params\n",
    "SHORT_MOM = 3\n",
    "MID_MOM = 5\n",
    "LONG_MOM = 10\n",
    "RSI_PERIOD = 14\n",
    "MACD_FAST = 12\n",
    "MACD_SLOW = 26\n",
    "MACD_SIGNAL = 9\n",
    "VOL_SHORT = 5\n",
    "VOL_LONG = 20\n",
    "BREAKOUT_SHORT = 20\n",
    "BREAKOUT_LONG = 50\n",
    "HORIZONS = [1, 2, 3]   # minutes ahead\n",
    "THRESHOLD = 0.001      # 0.1% move\n",
    "\n",
    "# --------------- HELPERS ---------------\n",
    "def ema(s, span):\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def rolling_zscore(s, window=50):\n",
    "    mu = s.rolling(window).mean()\n",
    "    sd = s.rolling(window).std().replace(0, np.nan)\n",
    "    return ((s - mu) / sd).fillna(0)\n",
    "\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0.0)\n",
    "    down = -1 * delta.clip(upper=0.0)\n",
    "    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = roll_up / (roll_down + 1e-12)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def compute_macd(series, fast=12, slow=26, signal=9):\n",
    "    fast_ema = ema(series, fast)\n",
    "    slow_ema = ema(series, slow)\n",
    "    macd = fast_ema - slow_ema\n",
    "    macd_signal = ema(macd, signal)\n",
    "    macd_hist = macd - macd_signal\n",
    "    return macd, macd_signal, macd_hist\n",
    "\n",
    "# --------------- LOAD & PREP ---------------\n",
    "# Read CSVs\n",
    "df_opt = pd.read_csv(OPTION_FILE)\n",
    "df_spot = pd.read_csv(SPOT_FILE)\n",
    "\n",
    "# Parse timestamps\n",
    "df_opt['timestamp'] = pd.to_datetime(df_opt['timestamp'])\n",
    "df_spot['timestamp'] = pd.to_datetime(df_spot['timestamp'])\n",
    "\n",
    "# Standardize column names if needed\n",
    "for c in ['open','high','low','close']:\n",
    "    if c not in df_spot.columns:\n",
    "        raise ValueError(f\"Spot CSV must contain column '{c}'\")\n",
    "\n",
    "required_opt_cols = ['nifty_ltp','atm_ce_ltp','atm_pe_ltp','ce_symbol','pe_symbol','atm_strike','expiry']\n",
    "for c in required_opt_cols:\n",
    "    if c not in df_opt.columns:\n",
    "        raise ValueError(f\"Option CSV must contain column '{c}'\")\n",
    "\n",
    "# Set index and sort\n",
    "df_spot = df_spot.sort_values('timestamp').set_index('timestamp')\n",
    "df_opt  = df_opt.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "# Ensure minute frequency continuous timeline (resample to 1min for spot)\n",
    "# We'll create a continuous index from earliest to latest minute using spot timestamps\n",
    "start = min(df_spot.index.min(), df_opt.index.min())\n",
    "end   = max(df_spot.index.max(), df_opt.index.max())\n",
    "# Align to minute boundaries\n",
    "start = start.floor('T')\n",
    "end = end.ceil('T')\n",
    "\n",
    "full_index = pd.date_range(start=start, end=end, freq='1T')  # 1-minute freq\n",
    "\n",
    "# Reindex spot to this continuous timeline using minute-level OHLC forward/backfill as appropriate\n",
    "# If spot already 1-minute candles, reindex will keep them; if missing minutes, we'll forward-fill last candle (option: can also mark as NaN)\n",
    "df_spot = df_spot.reindex(full_index)\n",
    "\n",
    "# For missing spot candles: forward fill close/open/high/low with last known candle (keeps continuity)\n",
    "# Better approach: for missing minute, we set open = prev close, high = max(prev close, next exist), low = min(prev close, next exist), close = prev close\n",
    "df_spot['close'] = df_spot['close'].ffill()\n",
    "df_spot['open']  = df_spot['open'].fillna(df_spot['close'])\n",
    "df_spot['high']  = df_spot['high'].fillna(df_spot['close'])\n",
    "df_spot['low']   = df_spot['low'].fillna(df_spot['close'])\n",
    "\n",
    "# Reindex / align options to same timeline using nearest tick within 1 minute\n",
    "# We'll use merge_asof on sorted indexes (merge_asof requires columns, so reset index)\n",
    "opt_reset = df_opt.reset_index().rename(columns={'timestamp':'ts'})\n",
    "spot_reset = df_spot.reset_index().rename(columns={'index':'timestamp'})\n",
    "\n",
    "# merge_asof requires sorted by key\n",
    "opt_reset = opt_reset.sort_values('ts')\n",
    "spot_reset = spot_reset.sort_values('timestamp')\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    spot_reset,\n",
    "    opt_reset,\n",
    "    left_on='timestamp',\n",
    "    right_on='ts',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('1min')  # keep option that is within +/- 1 minute\n",
    ")\n",
    "\n",
    "# If some rows didn't find an option tick within tolerance, forward-fill option columns so every minute has option values\n",
    "merged = merged.ffill()\n",
    "\n",
    "# Clean up: drop helper ts column\n",
    "if 'ts' in merged.columns:\n",
    "    merged = merged.drop(columns=['ts'])\n",
    "\n",
    "# Move timestamp to index\n",
    "merged['timestamp'] = pd.to_datetime(merged['timestamp'])\n",
    "merged = merged.set_index('timestamp').sort_index()\n",
    "\n",
    "# Basic price series\n",
    "price = merged['close'].astype(float)\n",
    "ce = merged['atm_ce_ltp'].astype(float)\n",
    "pe = merged['atm_pe_ltp'].astype(float)\n",
    "\n",
    "# --------------- FEATURE ENGINEERING ---------------\n",
    "# Basic returns\n",
    "merged['nifty_ret_1'] = price.pct_change().fillna(0)\n",
    "merged['ce_ret_1']    = ce.pct_change().fillna(0)\n",
    "merged['pe_ret_1']    = pe.pct_change().fillna(0)\n",
    "\n",
    "# Momentum features\n",
    "merged[f'mom_{SHORT_MOM}'] = price.pct_change(periods=SHORT_MOM)\n",
    "merged[f'mom_{MID_MOM}']   = price.pct_change(periods=MID_MOM)\n",
    "merged[f'mom_{LONG_MOM}']  = price.pct_change(periods=LONG_MOM)\n",
    "\n",
    "# Volatility (std of returns)\n",
    "merged['vol_short'] = merged['nifty_ret_1'].rolling(VOL_SHORT).std().fillna(0)\n",
    "merged['vol_long']  = merged['nifty_ret_1'].rolling(VOL_LONG).std().fillna(0)\n",
    "merged['vol_spike_ratio'] = (merged['vol_short'] / (merged['vol_long'] + 1e-12)).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# ATR using true range (uses spot high/low/close)\n",
    "high = merged['high'].astype(float).fillna(method='ffill')\n",
    "low  = merged['low'].astype(float).fillna(method='ffill')\n",
    "prev_close = merged['close'].shift(1).fillna(method='ffill')\n",
    "tr1 = (high - low).abs()\n",
    "tr2 = (high - prev_close).abs()\n",
    "tr3 = (low - prev_close).abs()\n",
    "true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "merged['atr_14'] = true_range.rolling(14).mean().fillna(0)\n",
    "\n",
    "# Breakouts and ranges\n",
    "merged['rolling_max_20'] = price.rolling(BREAKOUT_SHORT).max()\n",
    "merged['rolling_min_20'] = price.rolling(BREAKOUT_SHORT).min()\n",
    "merged['rolling_max_50'] = price.rolling(BREAKOUT_LONG).max()\n",
    "merged['rolling_min_50'] = price.rolling(BREAKOUT_LONG).min()\n",
    "\n",
    "merged['breakout_up_20'] = (price > merged['rolling_max_20'].shift(1)).astype(int)\n",
    "merged['breakout_down_20'] = (price < merged['rolling_min_20'].shift(1)).astype(int)\n",
    "\n",
    "range_20 = merged['rolling_max_20'] - merged['rolling_min_20']\n",
    "range_50 = merged['rolling_max_50'] - merged['rolling_min_50']\n",
    "merged['range_contraction'] = (range_20 / (range_50 + 1e-12)).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# RSI\n",
    "merged['rsi_14'] = compute_rsi(price, period=RSI_PERIOD)\n",
    "\n",
    "# MACD\n",
    "macd, macd_signal, macd_hist = compute_macd(price, MACD_FAST, MACD_SLOW, MACD_SIGNAL)\n",
    "merged['macd'] = macd\n",
    "merged['macd_signal'] = macd_signal\n",
    "merged['macd_hist'] = macd_hist\n",
    "\n",
    "# VWAP proxy: since no volume column in spot, use intraday typical price rolling cumulative per day\n",
    "typical_price = (merged['high'] + merged['low'] + merged['close']) / 3.0\n",
    "# compute running daily mean as a VWAP-like proxy\n",
    "merged['dtt'] = merged.index.date\n",
    "merged['typ_pv'] = typical_price  # no volume weight available\n",
    "merged['cum_typ_p'] = merged.groupby('dtt')['typ_pv'].cumsum()\n",
    "merged['cum_cnt'] = merged.groupby('dtt').cumcount() + 1\n",
    "merged['vwap'] = (merged['cum_typ_p'] / merged['cum_cnt']).fillna(method='ffill')\n",
    "merged = merged.drop(columns=['typ_pv','cum_typ_p','cum_cnt','dtt'])\n",
    "\n",
    "merged['vwap_dist'] = (price - merged['vwap']) / merged['vwap']\n",
    "merged['vwap_slope'] = merged['vwap'].diff().fillna(0)\n",
    "\n",
    "# CE/PE features\n",
    "merged['ce_pe_ratio'] = (ce / (pe + 1e-12)).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "merged['ce_minus_pe'] = ce - pe\n",
    "merged['ce_pe_delta'] = (ce.pct_change() - pe.pct_change()).fillna(0)\n",
    "merged['ce_volatility'] = merged['ce_ret_1'].rolling(VOL_LONG).std().fillna(0)\n",
    "merged['pe_volatility'] = merged['pe_ret_1'].rolling(VOL_LONG).std().fillna(0)\n",
    "\n",
    "# Liquidity proxy: use absolute minute returns averaged (since we don't have volume)\n",
    "merged['nifty_ret_abs'] = merged['nifty_ret_1'].abs().fillna(0)\n",
    "merged['avg_liq_20'] = merged['nifty_ret_abs'].rolling(20).mean().fillna(0)\n",
    "merged['liquidity_proxy'] = 1.0 / (merged['avg_liq_20'] + 1e-12)  # higher => more liquid proxy\n",
    "\n",
    "# Trend filters (ema)\n",
    "merged['nifty_ema_50'] = ema(price, 50)\n",
    "merged['nifty_ema_200'] = ema(price, 200)\n",
    "merged['trend_up'] = (merged['nifty_ema_50'] > merged['nifty_ema_200']).astype(int)\n",
    "merged['trend_down'] = (merged['nifty_ema_50'] < merged['nifty_ema_200']).astype(int)\n",
    "\n",
    "# Additional engineered features\n",
    "merged['momentum_ratio'] = (merged[f'mom_{SHORT_MOM}'] / (merged[f'mom_{LONG_MOM}'].abs() + 1e-12)).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "merged['volatility_weighted_mom'] = merged['momentum_ratio'] / (merged['vol_long'] + 1e-12)\n",
    "\n",
    "# Rolling z-scores\n",
    "merged['z_mom_3'] = rolling_zscore(merged[f'mom_{SHORT_MOM}'], 50)\n",
    "merged['z_vol_short'] = rolling_zscore(merged['vol_short'], 50)\n",
    "merged['z_ce_pe_ratio'] = rolling_zscore(merged['ce_pe_ratio'], 50)\n",
    "\n",
    "# --------------- TARGET LABELS ---------------\n",
    "for h in HORIZONS:\n",
    "    up_col = f'target_up_{h}m'\n",
    "    down_col = f'target_down_{h}m'\n",
    "    future_price = price.shift(-h)\n",
    "    ret = (future_price - price) / price\n",
    "    merged[up_col] = (ret >= THRESHOLD).astype(int)\n",
    "    merged[down_col] = (ret <= -THRESHOLD).astype(int)\n",
    "\n",
    "merged['target_1m_direction'] = 0\n",
    "merged.loc[merged['target_up_1m'] == 1, 'target_1m_direction'] = 1\n",
    "merged.loc[merged['target_down_1m'] == 1, 'target_1m_direction'] = -1\n",
    "\n",
    "# --------------- FINAL CLEAN & SAVE ---------------\n",
    "# Select feature columns (keep important ones)\n",
    "feature_cols = [\n",
    "    'open','high','low','close','nifty_ltp','atm_strike','ce_symbol','pe_symbol','atm_ce_ltp','atm_pe_ltp',\n",
    "    'nifty_ret_1','ce_ret_1','pe_ret_1',\n",
    "    f'mom_{SHORT_MOM}', f'mom_{MID_MOM}', f'mom_{LONG_MOM}',\n",
    "    'vol_short','vol_long','vol_spike_ratio','atr_14',\n",
    "    'breakout_up_20','breakout_down_20','range_contraction',\n",
    "    'rsi_14','macd','macd_signal','macd_hist',\n",
    "    'vwap','vwap_dist','vwap_slope',\n",
    "    'ce_pe_ratio','ce_minus_pe','ce_pe_delta','ce_volatility','pe_volatility',\n",
    "    'avg_liq_20','liquidity_proxy',\n",
    "    'nifty_ema_50','nifty_ema_200','trend_up','trend_down',\n",
    "    'momentum_ratio','volatility_weighted_mom','z_mom_3','z_vol_short','z_ce_pe_ratio'\n",
    "]\n",
    "\n",
    "label_cols = [f'target_up_{h}m' for h in HORIZONS] + [f'target_down_{h}m' for h in HORIZONS] + ['target_1m_direction']\n",
    "\n",
    "# keep only columns present\n",
    "final_cols = [c for c in feature_cols if c in merged.columns] + [c for c in label_cols if c in merged.columns]\n",
    "\n",
    "df_final = merged[final_cols].dropna().copy()\n",
    "\n",
    "# Save final features CSV\n",
    "df_final.to_csv(OUTPUT_FILE, index=True)\n",
    "print(f\"Saved merged features to {OUTPUT_FILE}\")\n",
    "print(\"Rows:\", len(df_final))\n",
    "print(\"Columns:\", df_final.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d532a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Files...\n",
      "2. Standardizing Timestamps...\n",
      "3. Merging Spot OHLC into Option Data...\n",
      "4. Removing extra timestamps (Keeping 09:15 - 15:30)...\n",
      "5. Saving to nifty_final_clean_1m.csv...\n",
      "------------------------------\n",
      " SUCCESS! File saved: nifty_final_clean_1m.csv\n",
      "Total Rows: 7598\n",
      "Columns: ['timestamp', 'nifty_ltp', 'expiry', 'atm_strike', 'ce_symbol', 'pe_symbol', 'atm_ce_ltp', 'atm_pe_ltp', 'open', 'high', 'low', 'close']\n",
      "            timestamp  nifty_ltp   expiry  atm_strike  \\\n",
      "0 2025-11-07 14:13:00   25473.70  11Nov25       25450   \n",
      "1 2025-11-07 14:14:00   25474.00  11Nov25       25450   \n",
      "2 2025-11-07 14:15:00   25470.55  11Nov25       25450   \n",
      "3 2025-11-07 14:16:00   25479.70  11Nov25       25500   \n",
      "4 2025-11-07 14:17:00   25474.60  11Nov25       25450   \n",
      "\n",
      "                    ce_symbol                   pe_symbol  atm_ce_ltp  \\\n",
      "0  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      109.80   \n",
      "1  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      111.70   \n",
      "2  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      109.60   \n",
      "3  NSE-NIFTY-11Nov25-25500-CE  NSE-NIFTY-11Nov25-25500-PE       85.85   \n",
      "4  NSE-NIFTY-11Nov25-25450-CE  NSE-NIFTY-11Nov25-25450-PE      111.70   \n",
      "\n",
      "   atm_pe_ltp      open      high       low     close  \n",
      "0       72.70  25472.70  25475.05  25471.65  25473.70  \n",
      "1       71.55  25474.35  25476.35  25472.60  25474.00  \n",
      "2       73.40  25474.95  25480.40  25469.30  25470.55  \n",
      "3       93.50  25470.45  25481.80  25470.45  25479.70  \n",
      "4       71.00  25480.20  25483.45  25473.65  25474.60  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OPTION_FILE = \"Final_option_ltp_1m_rolling.csv\"\n",
    "SPOT_FILE   = \"nifty_spot_1m.csv\"\n",
    "OUTPUT_FILE = \"nifty_final_clean_1m.csv\"\n",
    "\n",
    "def clean_and_merge():\n",
    "    print(\"1. Loading Files...\")\n",
    "    try:\n",
    "        df_opt = pd.read_csv(OPTION_FILE)\n",
    "        df_spot = pd.read_csv(SPOT_FILE)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Standardize Timestamps (No column name changes)\n",
    "    print(\"2. Standardizing Timestamps...\")\n",
    "    def to_dt(series):\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            return pd.to_datetime(series, unit='s')\n",
    "        return pd.to_datetime(series)\n",
    "\n",
    "    df_opt['timestamp'] = to_dt(df_opt['timestamp'])\n",
    "    df_spot['timestamp'] = to_dt(df_spot['timestamp'])\n",
    "\n",
    "    # 3. Merge Files\n",
    "    # We bring 'open', 'high', 'low', 'close' from the Spot file.\n",
    "    # We join on 'timestamp'.\n",
    "    print(\"3. Merging Spot OHLC into Option Data...\")\n",
    "    \n",
    "    # Select only the columns we need from Spot file\n",
    "    spot_data = df_spot[['timestamp', 'open', 'high', 'low', 'close']]\n",
    "    \n",
    "    # Merge (Inner join keeps only matching timestamps)\n",
    "    df = pd.merge(df_opt, spot_data, on='timestamp', how='inner')\n",
    "\n",
    "    # 4. Filter Time (Strictly 09:15 to 15:30)\n",
    "    print(\"4. Removing extra timestamps (Keeping 09:15 - 15:30)...\")\n",
    "    times = df['timestamp'].dt.time\n",
    "    start_time = pd.to_datetime(\"09:15:00\").time()\n",
    "    end_time   = pd.to_datetime(\"15:30:00\").time()\n",
    "    \n",
    "    # Keep rows strictly within market hours\n",
    "    df = df[(times >= start_time) & (times <= end_time)]\n",
    "\n",
    "    # 5. Save\n",
    "    print(f\"5. Saving to {OUTPUT_FILE}...\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\" SUCCESS! File saved: {OUTPUT_FILE}\")\n",
    "    print(f\"Total Rows: {len(df)}\")\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_and_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0d4ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\2142607532.py:56: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[\"ce_ret_1\"] = ce.pct_change().fillna(0)\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_9728\\2142607532.py:110: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[\"ce_pe_delta\"] = ce.pct_change() - pe.pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed  features_final.csv\n",
      "                     nifty_ltp   expiry  atm_strike  \\\n",
      "timestamp                                             \n",
      "2025-11-07 15:05:00   25479.95  11Nov25       25500   \n",
      "2025-11-07 15:06:00   25475.90  11Nov25       25500   \n",
      "2025-11-07 15:07:00   25479.85  11Nov25       25500   \n",
      "2025-11-07 15:08:00   25482.75  11Nov25       25500   \n",
      "2025-11-07 15:09:00   25482.15  11Nov25       25500   \n",
      "\n",
      "                                      ce_symbol                   pe_symbol  \\\n",
      "timestamp                                                                     \n",
      "2025-11-07 15:05:00  NSE-NIFTY-11Nov25-25500-CE  NSE-NIFTY-11Nov25-25500-PE   \n",
      "2025-11-07 15:06:00  NSE-NIFTY-11Nov25-25500-CE  NSE-NIFTY-11Nov25-25500-PE   \n",
      "2025-11-07 15:07:00  NSE-NIFTY-11Nov25-25500-CE  NSE-NIFTY-11Nov25-25500-PE   \n",
      "2025-11-07 15:08:00  NSE-NIFTY-11Nov25-25500-CE  NSE-NIFTY-11Nov25-25500-PE   \n",
      "2025-11-07 15:09:00  NSE-NIFTY-11Nov25-25500-CE  NSE-NIFTY-11Nov25-25500-PE   \n",
      "\n",
      "                     atm_ce_ltp  atm_pe_ltp      open      high       low  \\\n",
      "timestamp                                                                   \n",
      "2025-11-07 15:05:00       89.75       89.95  25481.95  25482.50  25478.95   \n",
      "2025-11-07 15:06:00       88.30       91.40  25480.95  25481.10  25474.00   \n",
      "2025-11-07 15:07:00       91.15       88.35  25477.00  25481.80  25475.70   \n",
      "2025-11-07 15:08:00       92.05       87.40  25480.90  25486.65  25480.35   \n",
      "2025-11-07 15:09:00       91.25       88.20  25483.20  25483.65  25478.70   \n",
      "\n",
      "                     ...   z_mom_3  z_vol_short  z_ce_pe_ratio  target_up_1m  \\\n",
      "timestamp            ...                                                       \n",
      "2025-11-07 15:05:00  ... -0.056060     0.921300      -1.197860             0   \n",
      "2025-11-07 15:06:00  ...  0.140462     0.320525      -1.353938             0   \n",
      "2025-11-07 15:07:00  ... -0.204879     0.512254      -1.016878             0   \n",
      "2025-11-07 15:08:00  ...  0.269685     0.055166      -0.934806             0   \n",
      "2025-11-07 15:09:00  ...  0.611336    -0.512202      -1.027627             0   \n",
      "\n",
      "                     target_down_1m  target_up_2m  target_down_2m  \\\n",
      "timestamp                                                           \n",
      "2025-11-07 15:05:00               0             0               0   \n",
      "2025-11-07 15:06:00               0             0               0   \n",
      "2025-11-07 15:07:00               0             0               0   \n",
      "2025-11-07 15:08:00               0             0               0   \n",
      "2025-11-07 15:09:00               0             0               0   \n",
      "\n",
      "                     target_up_3m  target_down_3m  target_1m_direction  \n",
      "timestamp                                                               \n",
      "2025-11-07 15:05:00             0               0                    0  \n",
      "2025-11-07 15:06:00             0               0                    0  \n",
      "2025-11-07 15:07:00             0               0                    0  \n",
      "2025-11-07 15:08:00             0               0                    0  \n",
      "2025-11-07 15:09:00             0               0                    0  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT = \"nifty_final_clean_1m.csv\"\n",
    "OUTPUT = \"features_final.csv\"\n",
    "\n",
    "RSI_PERIOD = 14\n",
    "MACD_FAST = 12\n",
    "MACD_SLOW = 26\n",
    "MACD_SIGNAL = 9\n",
    "SHORT_MOM = 3\n",
    "MID_MOM = 5\n",
    "LONG_MOM = 10\n",
    "VOL_SHORT = 5\n",
    "VOL_LONG = 20\n",
    "BREAKOUT_SHORT = 20\n",
    "BREAKOUT_LONG = 50\n",
    "HORIZONS = [1, 2, 3]\n",
    "THRESHOLD = 0.001  # 0.1%\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "def ema(s, span):\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = roll_up / (roll_down + 1e-12)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(series, fast, slow, signal):\n",
    "    fast_ema = ema(series, fast)\n",
    "    slow_ema = ema(series, slow)\n",
    "    macd = fast_ema - slow_ema\n",
    "    macd_signal = ema(macd, signal)\n",
    "    macd_hist = macd - macd_signal\n",
    "    return macd, macd_signal, macd_hist\n",
    "\n",
    "# ---------------- LOAD ----------------\n",
    "df = pd.read_csv(INPUT)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "df = df.sort_index()\n",
    "\n",
    "# PRICE SERIES\n",
    "price = df[\"close\"]\n",
    "ce = df[\"atm_ce_ltp\"]\n",
    "pe = df[\"atm_pe_ltp\"]\n",
    "\n",
    "# ---------------- BASIC RETURNS ----------------\n",
    "df[\"nifty_ret_1\"] = price.pct_change().fillna(0)\n",
    "df[\"ce_ret_1\"] = ce.pct_change().fillna(0)\n",
    "df[\"pe_ret_1\"] = pe.pct_change().fillna(0)\n",
    "\n",
    "# ---------------- MOMENTUM ----------------\n",
    "df[f\"mom_{SHORT_MOM}\"] = price.pct_change(SHORT_MOM)\n",
    "df[f\"mom_{MID_MOM}\"] = price.pct_change(MID_MOM)\n",
    "df[f\"mom_{LONG_MOM}\"] = price.pct_change(LONG_MOM)\n",
    "\n",
    "# ---------------- VOLATILITY ----------------\n",
    "df[\"vol_short\"] = df[\"nifty_ret_1\"].rolling(VOL_SHORT).std().fillna(0)\n",
    "df[\"vol_long\"] = df[\"nifty_ret_1\"].rolling(VOL_LONG).std().fillna(0)\n",
    "df[\"vol_spike_ratio\"] = (df[\"vol_short\"] / (df[\"vol_long\"] + 1e-12)).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# ---------------- ATR ----------------\n",
    "high = df[\"high\"]\n",
    "low = df[\"low\"]\n",
    "prev_close = price.shift(1)\n",
    "\n",
    "tr = pd.concat([\n",
    "    (high - low).abs(),\n",
    "    (high - prev_close).abs(),\n",
    "    (low - prev_close).abs()\n",
    "], axis=1).max(axis=1)\n",
    "\n",
    "df[\"atr_14\"] = tr.rolling(14).mean().fillna(0)\n",
    "\n",
    "# ---------------- BREAKOUTS ----------------\n",
    "df[\"rolling_max_20\"] = price.rolling(BREAKOUT_SHORT).max()\n",
    "df[\"rolling_min_20\"] = price.rolling(BREAKOUT_SHORT).min()\n",
    "\n",
    "df[\"breakout_up_20\"] = (price > df[\"rolling_max_20\"].shift(1)).astype(int)\n",
    "df[\"breakout_down_20\"] = (price < df[\"rolling_min_20\"].shift(1)).astype(int)\n",
    "\n",
    "df[\"range_contraction\"] = (\n",
    "    (df[\"rolling_max_20\"] - df[\"rolling_min_20\"]) /\n",
    "    ((df[\"rolling_max_20\"].rolling(BREAKOUT_LONG).max() -\n",
    "      df[\"rolling_min_20\"].rolling(BREAKOUT_LONG).min()) + 1e-12)\n",
    ").fillna(0)\n",
    "\n",
    "# ---------------- RSI ----------------\n",
    "df[\"rsi_14\"] = compute_rsi(price, RSI_PERIOD)\n",
    "\n",
    "# ---------------- MACD ----------------\n",
    "df[\"macd\"], df[\"macd_signal\"], df[\"macd_hist\"] = compute_macd(price, MACD_FAST, MACD_SLOW, MACD_SIGNAL)\n",
    "\n",
    "# ---------------- VWAP PROXY ----------------\n",
    "tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "df[\"vwap\"] = tp.rolling(20).mean()  # proxy\n",
    "df[\"vwap_dist\"] = (price - df[\"vwap\"]) / df[\"vwap\"]\n",
    "df[\"vwap_slope\"] = df[\"vwap\"].diff().fillna(0)\n",
    "\n",
    "# ---------------- CE/PE FEATURES ----------------\n",
    "df[\"ce_pe_ratio\"] = ce / (pe + 1e-12)\n",
    "df[\"ce_minus_pe\"] = ce - pe\n",
    "df[\"ce_pe_delta\"] = ce.pct_change() - pe.pct_change()\n",
    "df[\"ce_volatility\"] = df[\"ce_ret_1\"].rolling(VOL_LONG).std().fillna(0)\n",
    "df[\"pe_volatility\"] = df[\"pe_ret_1\"].rolling(VOL_LONG).std().fillna(0)\n",
    "\n",
    "# ---------------- LIQUIDITY PROXY ----------------\n",
    "df[\"avg_liq_20\"] = df[\"nifty_ret_1\"].abs().rolling(20).mean().fillna(0)\n",
    "df[\"liquidity_proxy\"] = 1 / (df[\"avg_liq_20\"] + 1e-12)\n",
    "\n",
    "# ---------------- TREND FILTER ----------------\n",
    "df[\"nifty_ema_50\"] = ema(price, 50)\n",
    "df[\"nifty_ema_200\"] = ema(price, 200)\n",
    "df[\"trend_up\"] = (df[\"nifty_ema_50\"] > df[\"nifty_ema_200\"]).astype(int)\n",
    "df[\"trend_down\"] = (df[\"nifty_ema_50\"] < df[\"nifty_ema_200\"]).astype(int)\n",
    "\n",
    "# ---------------- ADVANCED FEATURES ----------------\n",
    "df[\"momentum_ratio\"] = df[f\"mom_{SHORT_MOM}\"] / (df[f\"mom_{LONG_MOM}\"].abs() + 1e-12)\n",
    "df[\"volatility_weighted_mom\"] = df[\"momentum_ratio\"] / (df[\"vol_long\"] + 1e-12)\n",
    "\n",
    "df[\"z_mom_3\"] = (df[f\"mom_{SHORT_MOM}\"] - df[f\"mom_{SHORT_MOM}\"].rolling(50).mean()) / \\\n",
    "                (df[f\"mom_{SHORT_MOM}\"].rolling(50).std() + 1e-12)\n",
    "\n",
    "df[\"z_vol_short\"] = (df[\"vol_short\"] - df[\"vol_short\"].rolling(50).mean()) / \\\n",
    "                    (df[\"vol_short\"].rolling(50).std() + 1e-12)\n",
    "\n",
    "df[\"z_ce_pe_ratio\"] = (df[\"ce_pe_ratio\"] - df[\"ce_pe_ratio\"].rolling(50).mean()) / \\\n",
    "                      (df[\"ce_pe_ratio\"].rolling(50).std() + 1e-12)\n",
    "\n",
    "# ---------------- TARGETS ----------------\n",
    "for h in HORIZONS:\n",
    "    future_price = price.shift(-h)\n",
    "    ret = (future_price - price) / price\n",
    "    df[f\"target_up_{h}m\"] = (ret >= THRESHOLD).astype(int)\n",
    "    df[f\"target_down_{h}m\"] = (ret <= -THRESHOLD).astype(int)\n",
    "\n",
    "df[\"target_1m_direction\"] = 0\n",
    "df.loc[df[\"target_up_1m\"] == 1, \"target_1m_direction\"] = 1\n",
    "df.loc[df[\"target_down_1m\"] == 1, \"target_1m_direction\"] = -1\n",
    "\n",
    "# ---------------- CLEAN ----------------\n",
    "df = df.dropna()\n",
    "\n",
    "# ---------------- SAVE ----------------\n",
    "df.to_csv(OUTPUT)\n",
    "print(\"Feature extraction completed \", OUTPUT)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b8f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 7496\n",
      "Target up positives: 131 Rate: 0.01747598719316969\n",
      "Target down negatives: 130 Rate: 0.017342582710779084\n",
      "Saved with new targets to features_final_with_target.csv\n"
     ]
    }
   ],
   "source": [
    "# update_targets_0p05.py\n",
    "# Creates target_up_1m_0p05 and target_1m_direction_0p05 in merged_features_final.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT = \"features_final.csv\"\n",
    "OUTPUT = \"features_final_with_target.csv\"\n",
    "\n",
    "THRESHOLD = 0.0005  # 0.05% = ~10 points at NIFTY 20k-22k\n",
    "HORIZON = 1  # minutes\n",
    "\n",
    "df = pd.read_csv(INPUT)\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "# use 'close' as base price\n",
    "price = df['close'].astype(float)\n",
    "\n",
    "# target up if future price increases at least THRESHOLD in HORIZON minutes\n",
    "future_price = price.shift(-HORIZON)\n",
    "ret = (future_price - price) / price\n",
    "\n",
    "df[f\"target_up_{HORIZON}m_0p05\"] = (ret >= THRESHOLD).astype(int)\n",
    "df[f\"target_down_{HORIZON}m_0p05\"] = (ret <= -THRESHOLD).astype(int)\n",
    "\n",
    "# direction label\n",
    "df[f\"target_1m_direction_0p05\"] = 0\n",
    "df.loc[df[f\"target_up_{HORIZON}m_0p05\"] == 1, f\"target_1m_direction_0p05\"] = 1\n",
    "df.loc[df[f\"target_down_{HORIZON}m_0p05\"] == 1, f\"target_1m_direction_0p05\"] = -1\n",
    "\n",
    "# drop final rows with NaN targets (warmup at end)\n",
    "df = df.dropna()\n",
    "\n",
    "# summary\n",
    "total = len(df)\n",
    "pos = df[f\"target_up_{HORIZON}m_0p05\"].sum()\n",
    "neg = df[f\"target_down_{HORIZON}m_0p05\"].sum()\n",
    "print(\"Rows:\", total)\n",
    "print(\"Target up positives:\", pos, \"Rate:\", pos/total)\n",
    "print(\"Target down negatives:\", neg, \"Rate:\", neg/total)\n",
    "\n",
    "# save\n",
    "df.to_csv(OUTPUT, index=True)\n",
    "print(\"Saved with new targets to\", OUTPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dc59da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 1m movement (points): 4.037991994663116\n",
      "Median 1m movement (points): 2.849999999998545\n",
      "Average 1m movement (%): 0.00015537391192760994\n",
      "Median 1m movement (%): 0.00010902705783784214\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"features_final.csv\", parse_dates=[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "# 1-minute absolute returns\n",
    "df[\"ret_1m_abs\"] = (df[\"close\"].shift(-1) - df[\"close\"]).abs()\n",
    "\n",
    "# Average movement\n",
    "avg_points = df[\"ret_1m_abs\"].mean()\n",
    "median_points = df[\"ret_1m_abs\"].median()\n",
    "\n",
    "# Percent returns\n",
    "df[\"ret_1m_pct\"] = df[\"ret_1m_abs\"] / df[\"close\"]\n",
    "\n",
    "avg_pct = df[\"ret_1m_pct\"].mean()\n",
    "median_pct = df[\"ret_1m_pct\"].median()\n",
    "\n",
    "print(\"Average 1m movement (points):\", avg_points)\n",
    "print(\"Median 1m movement (points):\", median_points)\n",
    "print(\"Average 1m movement (%):\", avg_pct)\n",
    "print(\"Median 1m movement (%):\", median_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a0e2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    7495.000000\n",
      "mean        4.037992\n",
      "std         4.965033\n",
      "min         0.000000\n",
      "25%         1.300000\n",
      "50%         2.850000\n",
      "75%         5.300000\n",
      "max       144.300000\n",
      "Name: ret_1m_abs, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"ret_1m_abs\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153a0a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Groww!\n",
      "Logged into Groww!\n",
      "Downloading interval: 1hour\n",
      "Saved nifty_spot_1hour.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from growwapi import GrowwAPI\n",
    "\n",
    "# --- CREDENTIALS ---\n",
    "API_KEY = \"eyJraWQiOiJaTUtjVXciLCJhbGciOiJFUzI1NiJ9.eyJleHAiOjI1NTM1Njg0NzYsImlhdCI6MTc2NTE2ODQ3NiwibmJmIjoxNzY1MTY4NDc2LCJzdWIiOiJ7XCJ0b2tlblJlZklkXCI6XCJhMTg3NDVhMy1hN2M1LTRlOTQtODE1MS1lZjUxZDQ5OGE2Y2RcIixcInZlbmRvckludGVncmF0aW9uS2V5XCI6XCJlMzFmZjIzYjA4NmI0MDZjODg3NGIyZjZkODQ5NTMxM1wiLFwidXNlckFjY291bnRJZFwiOlwiMDdmMDA0MGMtZTk4Zi00ZDNmLTk5Y2EtZDc1ZjBlYWU5M2NlXCIsXCJkZXZpY2VJZFwiOlwiZDMyMWIxMzUtZWQ5Mi01ZWJkLWJjMDUtZTY1NDY2OWRiMDM5XCIsXCJzZXNzaW9uSWRcIjpcIjBlOWMyYWZmLTM0NzktNDUyMi1iODE4LTczNTZlMzFkYmY1Y1wiLFwiYWRkaXRpb25hbERhdGFcIjpcIno1NC9NZzltdjE2WXdmb0gvS0EwYk1yOE5XVzhzdTNvZ080am1ZUzIwZEpSTkczdTlLa2pWZDNoWjU1ZStNZERhWXBOVi9UOUxIRmtQejFFQisybTdRPT1cIixcInJvbGVcIjpcImF1dGgtdG90cFwiLFwic291cmNlSXBBZGRyZXNzXCI6XCIyNDA5OjQwOTA6MTA4ZjpkYzA1OjNkMWQ6MWZmMDo1YWFjOjYwNTYsMTcyLjcxLjE5OC4xOSwzNS4yNDEuMjMuMTIzXCIsXCJ0d29GYUV4cGlyeVRzXCI6MjU1MzU2ODQ3NjQzNn0iLCJpc3MiOiJhcGV4LWF1dGgtcHJvZC1hcHAifQ.VuAMgqoC3e32gduObByNz97jFfG-ikXoREum26XPkvyMpj9JgCedXBI81jxGTPTrZD9i1wIL0s38LPd9vc9ApA\"\n",
    "API_SECRET = \"xy0sbQ4r*!HN3&&UKc9vpwti4xx8PR)(\"\n",
    "\n",
    "def auth():\n",
    "    try:\n",
    "        token = GrowwAPI.get_access_token(api_key=API_KEY, secret=API_SECRET)\n",
    "        return GrowwAPI(token)\n",
    "    except Exception as e:\n",
    "        print(f\"Auth failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "groww = auth()\n",
    "print(\"Logged into Groww!\")\n",
    "SYMBOL = \"NSE-NIFTY\" \n",
    "EXCHANGE = \"NSE\"          # Passed as string\n",
    "SEGMENT = \"CASH\"       # Passed as string (\"INDICES\" for Nifty 50, \"CASH\" for stocks)\n",
    "           # Passed as string (e.g., \"1m\", \"5m\", \"15m\")\n",
    "\n",
    "# --- CHUNKING LOGIC ---\n",
    "total_days = 3994\n",
    "chunk_size_days = 5\n",
    "end_date = pd.Timestamp.now()\n",
    "start_date = end_date - timedelta(days=total_days)\n",
    "\n",
    "all_candles = []\n",
    "current_start = start_date\n",
    "\n",
    "INTERVALS = [\"1hour\"]\n",
    "\n",
    "for INTERVAL in INTERVALS:\n",
    "    all_candles = []\n",
    "    current_start = start_date\n",
    "    print(\"Downloading interval:\", INTERVAL)\n",
    "    while current_start < end_date:\n",
    "        current_end = current_start + timedelta(days=chunk_size_days)\n",
    "        if current_end > end_date:\n",
    "            current_end = end_date\n",
    "        s_str = current_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        e_str = current_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        try:\n",
    "            resp = groww.get_historical_candles(\n",
    "                exchange=EXCHANGE,\n",
    "                segment=SEGMENT,\n",
    "                groww_symbol=SYMBOL,\n",
    "                start_time=s_str,\n",
    "                end_time=e_str,\n",
    "                candle_interval=INTERVAL\n",
    "            )\n",
    "            if resp and \"candles\" in resp and resp[\"candles\"]:\n",
    "                all_candles.extend(resp[\"candles\"])\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        current_start = current_end\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if all_candles:\n",
    "        df = pd.DataFrame(all_candles)\n",
    "        # same robust timestamp fix as you already have...\n",
    "        # save file name by interval\n",
    "        df.to_csv(f\"nifty_spot_{INTERVAL}.csv\", index=False)\n",
    "        print(\"Saved\", f\"nifty_spot_{INTERVAL}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
